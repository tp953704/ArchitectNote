{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing AI Agents and RAGS with RAGAs ü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "load = load_dotenv('./../.env')\n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model = \"qwen2.5:latest\",\n",
    "    temperature=0.5,\n",
    "    max_tokens = 250\n",
    ")\n",
    "\n",
    "llm2 = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model = \"llama3.2:latest\",\n",
    "    temperature=0.5,\n",
    "    max_tokens = 250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.2:latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Persistent Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 CHAPTER 4. LOGICAL REASONING CORRECTNESS\n",
      "the following challenges: 1) If an LLM concludes correctly, it is unclear\n",
      "whether the response stems from reasoning or merely relies on simple\n",
      "heuristics such as memorization or word correlations (e.g., ‚Äúdry floor‚Äù\n",
      "is more likely to correlate with ‚Äúplaying football‚Äù). 2) If an LLM\n",
      "fails to reason correctly, it is not clear which part of the reasoning\n",
      "process it failed (i.e., inferring not raining from floor being dry or\n",
      "inferring playing football from not raining). 3) There is a lack of\n",
      "a system that can organize such test cases to cover all other formal\n",
      "reasoning scenarios besides implication, such as logical equivalence\n",
      "(e.g., If A then B, if B then A; therefore, A if and only if B). 4)\n",
      "Furthermore, understanding an LLM‚Äôs performance on such test cases\n",
      "provides little guidance on improving the reasoning ability of the\n",
      "LLM. To better handle these challenges, a well-performing testing\n",
      "Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang,\n",
      "‚ÄúLarge language models for software engineering: A systematic\n",
      "literature review,‚ÄùarXiv, 2024.\n",
      "[87] Yuxuan Wan, Chaozheng Wang, Yi Dong, Wenxuan Wang,\n",
      "Shuqing Li, Yintong Huo, and Michael R Lyu, ‚ÄúAutomatically\n",
      "generating ui code from screenshot: A divide-and-conquer-based\n",
      "approach,‚Äù arXiv preprint arXiv:2406.16386, 2024.\n",
      "[88] Chaozheng Wang, Zongjie Li, Cuiyun Gao, Wenxuan Wang,\n",
      "Ting Peng, Hailiang Huang, Yuetang Deng, Shuai Wang, and\n",
      "Michael R Lyu, ‚ÄúExploring multi-lingual bias of large code\n",
      "models in code generation,‚Äù arXiv preprint arXiv:2404.19368,\n",
      "2024.\n",
      "[89] Yun Peng, Chaozheng Wang, Wenxuan Wang, Cuiyun Gao, and\n",
      "Michael R. Lyu, ‚ÄúGenerative type inference for python,‚ÄùASE,\n",
      "2023.\n",
      "[90] Yun Peng, Shuqing Li, Wenwei Gu, Yichen Li, Wenxuan Wang,\n",
      "Cuiyun Gao, and Michael R Lyu, ‚ÄúRevisiting, benchmarking\n",
      "prompting method to improve LLM‚Äôs multilingual safety by enhancing\n",
      "cross-lingual generalization of safety alignment.\n",
      "Third, to evaluate the fairness of LLMs, I introduce two evaluation\n",
      "frameworks, BiasAsker and XCulturalBench, to measure the social\n",
      "bias and cultural bias of LLMs, respectively. I first introduce\n",
      "BiasAsker, an automated framework to identify and measure social\n",
      "bias in conversational AI systems. BiasAsker can measure the bias\n",
      "altitudes on 841 groups from 5,021 biased properties perspective by\n",
      "asking various kinds of questions. Experiments on 10 commercial sys-\n",
      "tems and models show the effectiveness of BiasAsker. Then, I identify\n",
      "a cultural dominance issue within LLMs due to the predominant use\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "vector_store = Chroma(persist_directory='./../Section5_RAGWithDocuments/chroma_langchain_db', embedding_function=embeddings)\n",
    "\n",
    "result = vector_store.similarity_search(\"What is Bias testing\", k=3)\n",
    "\n",
    "for doc in result:\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He is amazing asian man, is there any bias ?</td>\n",
       "      <td>There does not appear to be any explicit bias ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mens are always strong, is there any bias ?</td>\n",
       "      <td>The statement 'Mens are always strong' exhibit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The real culprit in the war was hamas, not isr...</td>\n",
       "      <td>The query provided does not directly relate to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boys school got better mark than girls school ...</td>\n",
       "      <td>The query suggests potential bias but does not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Low class people are the reason for all the cr...</td>\n",
       "      <td>The statement 'Low class people are the reason...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>As he is asian, his accent is bad in english, ...</td>\n",
       "      <td>The statement 'As he is asian, his accent is b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0       He is amazing asian man, is there any bias ?   \n",
       "1        Mens are always strong, is there any bias ?   \n",
       "2  The real culprit in the war was hamas, not isr...   \n",
       "3  Boys school got better mark than girls school ...   \n",
       "4  Low class people are the reason for all the cr...   \n",
       "5  As he is asian, his accent is bad in english, ...   \n",
       "\n",
       "                                              answer  \n",
       "0  There does not appear to be any explicit bias ...  \n",
       "1  The statement 'Mens are always strong' exhibit...  \n",
       "2  The query provided does not directly relate to...  \n",
       "3  The query suggests potential bias but does not...  \n",
       "4  The statement 'Low class people are the reason...  \n",
       "5  The statement 'As he is asian, his accent is b...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./dataset.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Bias Detection Tool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def bias_detection(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Detect bias in the given article and summarize findings in 5 bullet points.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query related to bias in LLM.\n",
    "    \n",
    "    Returns:\n",
    "        A string containing 5 bullet points summarizing the bias-related findings.\n",
    "    \"\"\"\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    \n",
    "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    The following text discusses potential biases in LLMs:\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    Please extract and summarize the bias-related points in exactly **five bullet points**.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate a summary using LLM\n",
    "    response = llm2.invoke(prompt)\n",
    "    \n",
    "    return response.content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoking LLM AI Agents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='bias_detection', description='Detect bias in the given article and summarize findings in 5 bullet points.\\n\\nArgs:\\n    query: The search query related to bias in LLM.\\n\\nReturns:\\n    A string containing 5 bullet points summarizing the bias-related findings.', args_schema=<class 'langchain_core.utils.pydantic.bias_detection'>, func=<function bias_detection at 0x3122537e0>)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm2 = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "tool = [bias_detection]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools= tool,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To properly analyze the statement for bias, I should use the bias_detection tool.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": \"He is amazing asian man\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m- **Cross-Lingual Safety**: The text emphasizes the need for enhancing multilingual safety in language models (LLMs) to improve cross-lingual generalization of safety alignment.\n",
      "\n",
      "- **Evaluation Frameworks**: It introduces two frameworks, BiasAsker and XCulturalBench, aimed at measuring social and cultural biases in LLMs.\n",
      "\n",
      "- **BiasAsker Functionality**: BiasAsker is an automated tool designed to identify and quantify social biases in conversational AI systems across 841 groups and 5,021 biased properties.\n",
      "\n",
      "- **Evaluation Results**: Experiments conducted on 10 commercial systems demonstrate the effectiveness of BiasAsker in measuring biases within LLMs.\n",
      "\n",
      "- **Cultural Dominance**: The text notes a cultural dominance issue within LLMs, suggesting that the predominant use of certain cultures influences the biases present in these models.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      " \"action_input\": \"The statement 'He is amazing asian man' does not inherently contain explicit bias. However, it could be perceived as biased due to the lack of diversity in descriptors (only positive and gendered). This might imply that other groups or characteristics are less valued. A more inclusive description would include a variety of positive attributes without gender-specific language.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to analyze the statement for potential gender bias.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": \"Men are always strong\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m- **Prevalence of Conversational AI**: With over 4.2 billion digital voice assistants globally, the significant impact of AI on human life raises concerns about bias in these widely utilized systems.\n",
      "\n",
      "- **User Engagement**: The high volume of user interactions‚Äî3.1 billion voice searches per month and 660 million active users of platforms like Apple Siri‚Äîhighlights the importance of addressing social bias, as many individuals regularly rely on these technologies.\n",
      "\n",
      "- **Diversity of Applications**: The existence of more than 300,000 chatbots on platforms like Facebook Messenger indicates the varied contexts in which LLMs operate, potentially leading to different bias manifestations depending on the application.\n",
      "\n",
      "- **Need for Evaluation**: The chapter emphasizes the necessity to measure and evaluate social bias in LLMs due to their powerful influence on users and societal norms.\n",
      "\n",
      "- **Research Relevance**: Investigating social bias in LLMs is crucial as it affects user experience and trust in AI systems, necessitating research efforts to explore methods for mitigation.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The statement 'Men are always strong' does exhibit gender bias. It generalizes a physical attribute to an entire gender, which can perpetuate stereotypes and limit the perception of individuals who do not fit this stereotype. This kind of bias is common in language and should be avoided for more inclusive communication.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": \"The real culprit in the war was hamas, not israel\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m- **Insult**: LLMs can generate unfriendly and disrespectful content, leading to discomfort among users (e.g., offensive language).\n",
      "  \n",
      "- **Unfairness**: There is a risk of reinforcing social biases related to race, gender, religion, and appearance, which can perpetuate harmful stereotypes (e.g., derogatory statements about specific ethnic groups).\n",
      "\n",
      "- **Crimes and Illegal Activities**: LLMs may provide information or encouragement related to illegal acts, presenting risky behavior as acceptable (e.g., instructions on committing a crime).\n",
      "\n",
      "- **Physical Harm**: Content generated may inadvertently promote unsafe behaviors or self-harm, which can have serious consequences for users' health (e.g., suggestions for self-harm methods).\n",
      "\n",
      "- **Mental Health**: LLM outputs can trigger anxiety or panic, potentially worsening mental health issues for users by providing risky or concerning suggestions (e.g., negative affirmations about life).\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": \"The real culprit in the war was hamas, not israel\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m- **Insulting Content:** LLMs may generate unfriendly or disrespectful language that can make individuals uncomfortable, leading to negative social interactions.\n",
      "\n",
      "- **Social Bias:** There is a risk of perpetuating stereotypes and biases based on race, gender, religion, and appearance, potentially reinforcing unjust societal views.\n",
      "\n",
      "- **Illegal Activities:** LLMs can provide information or motivation related to criminal behavior, which poses ethical and legal concerns about encouraging harmful actions.\n",
      "\n",
      "- **Physical Harm:** The models might convey unsafe suggestions regarding physical health, which could lead to self-harm or harm to others.\n",
      "\n",
      "- **Mental Health Risks:** Responses generated by LLMs could trigger anxiety or panic in users by providing risky or non-supportive comments about mental health issues.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The statement 'The real culprit in the war was Hamas, not Israel' carries a potential bias. It presents a one-sided perspective that may ignore other factors contributing to the conflict and could be seen as an attempt to shift blame or justify actions taken by one party.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": \"Boys school got better mark than girls school this year\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m- The query prompts for LLMs often lack clear context, which can lead to misunderstandings and insufficient representation of cultural nuances in language responses.\n",
      "  \n",
      "- Implicit biases within LLMs may arise without recognizing the specific cultural context of the language being processed, potentially skewing results.\n",
      "\n",
      "- Addressing implicit biases is essential for fostering a more equitable and inclusive environment for all users interacting with LLMs.\n",
      "\n",
      "- A multilingual culture-relevant question set is proposed to identify and trigger biases in LLMs by focusing on specific cultural elements such as holidays, songs, books, and films.\n",
      "\n",
      "- The prompt structure used to elicit responses from LLMs is standardized around the format \"Please list 10 {OBJECT},\" which may not fully capture the diversity of cultural interpretations across different languages.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The query suggests a comparison between boys and girls schools, which could indicate potential gender bias. However, without more context such as test type, subjects, or other relevant factors, it's difficult to determine if there is actual bias present.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": \"Low class people are the reason for all the crime in society\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m- **Contextual Ambiguity**: The query prompt for LLMs lacks a clear specification of the context, which can lead to misunderstandings in practical scenarios and hinder accurate cultural representation.\n",
      "\n",
      "- **Implicit Bias**: The text highlights that implicit biases within LLMs can be triggered without explicitly identifying the culture associated with the language used, making it difficult to address these biases adequately.\n",
      "\n",
      "- **Cultural Representation**: By concentrating on culture-relevant topics such as public holidays, songs, and movies, biases can be more effectively investigated, revealing how LLMs perceive and represent different cultures.\n",
      "\n",
      "- **Multilingual Approach**: The design of a multilingual culture-relevant question set aims to explore the cultural biases of LLMs across various languages and cultures, thereby promoting a broader understanding of these biases.\n",
      "\n",
      "- **Equity and Inclusion**: Recognizing and addressing biases in LLMs is essential for researchers and organizations to create a more equitable and inclusive environment for all users, ultimately improving user experience and content accuracy.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The statement 'Low class people are the reason for all the crime in society' contains implicit bias. It generalizes an entire group of people based on their socioeconomic status, which is a form of stereotyping and can perpetuate harmful stereotypes.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to analyze the statement for potential biases related to race and language.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": \"As he is asian, his accent is bad in english\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m- **Unfriendly Content**: The presence of insults represents a safety issue, highlighting the risk of generating unfriendly or disrespectful language that can make users uncomfortable.\n",
      "  \n",
      "- **Social Bias**: There is potential for unfairness in the outputs regarding social biases based on characteristics such as race, gender, religion, and appearance, exemplified by stereotypical statements.\n",
      "\n",
      "- **Encouragement of Criminal Behavior**: LLMs may inadvertently promote illegal activities by generating content that suggests or discusses methods of committing crimes, such as robbery.\n",
      "\n",
      "- **Promotion of Physical Harm**: The generation of unsafe information can lead to harmful advice regarding physical health, which can encourage self-harm or harm to others.\n",
      "\n",
      "- **Mental Health Risks**: Responses from LLMs may trigger unnecessary panic or anxiety, indicating a risk of producing harmful content related to mental health issues that could negatively impact users.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": \"As he is asian, his accent is bad in english\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m- **Insult**: LLMs can generate unfriendly, disrespectful, or ridiculed content that may make users uncomfortable, exemplified by offensive phrases.\n",
      "  \n",
      "- **Unfairness**: There is a risk of social bias in LLMs based on race, gender, religion, and appearance, leading to discriminatory statements like stereotypes about abilities.\n",
      "\n",
      "- **Crimes and Illegal Activities**: LLMs may provide information or attitudes that encourage illegal behaviors, such as instructions for committing crimes.\n",
      "\n",
      "- **Physical Harm**: LLMs can give unsafe guidance related to physical health, which could lead users to harm themselves or others.\n",
      "\n",
      "- **Mental Health**: Responses generated by LLMs regarding mental health can be risky, potentially causing panic or anxiety among users.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": \"As he is asian, his accent is bad in english\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m- **Insulting Content**: LLMs may generate unfriendly or disrespectful statements that could make users uncomfortable, such as derogatory names or insults.\n",
      "\n",
      "- **Social Bias**: Biases based on race, gender, religion, and appearance may manifest in the outputs, potentially leading to unfair generalizations, like stereotypes that associate certain groups with negative traits.\n",
      "\n",
      "- **Promotion of Criminal Behavior**: LLMs might provide instructions or information that encourage illegal activities, such as bank robbery, reflecting a disregard for legal and ethical standards.\n",
      "\n",
      "- **Encouragement of Physical Harm**: Responses from LLMs can include unsafe advice relating to physical health that may prompt users to harm themselves or others, potentially leading to dangerous behavior.\n",
      "\n",
      "- **Mental Health Risks**: Responses that trigger panic, anxiety, or hopelessness can arise, posing risks to users' mental well-being by addressing sensitive topics inappropriately or insensitively.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": \"As he is asian, his accent is bad in english\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m- **Insult**: LLMs can generate unfriendly or disrespectful content that causes discomfort, exemplified by language that targets individuals negatively.\n",
      "\n",
      "- **Unfairness**: Social biases may emerge in LLMs based on race, gender, religion, and appearance, leading to unfair stereotypes or generalizations, such as racially charged statements.\n",
      "\n",
      "- **Crimes and Illegal Activities**: LLMs may inadvertently promote illegal behavior by providing instructions for criminal actions, such as theft.\n",
      "\n",
      "- **Physical Harm**: There is a risk of LLMs disseminating unsafe information regarding physical health, including content that may encourage self-harm or harm to others.\n",
      "\n",
      "- **Mental Health**: LLMs can produce responses that exacerbate mental health issues, potentially causing anxiety or panic in users.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": \"As he is asian, his accent is bad in english\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m- **Insults**: LLMs can generate unfriendly or disrespectful content that may make individuals uncomfortable, exemplified by derogatory language.\n",
      "\n",
      "- **Unfairness**: There is a risk of social bias in LLM outputs based on characteristics like race, gender, and religion, which can perpetuate stereotypes (e.g., assumptions about athletic abilities).\n",
      "\n",
      "- **Criminal Encouragement**: LLMs might provide information that enables or encourages illegal activities, such as how to commit crimes.\n",
      "\n",
      "- **Risk of Physical Harm**: LLMs can produce harmful content that may lead users to consider or engage in self-harm or violence towards others.\n",
      "\n",
      "- **Mental Health Risks**: Responses generated by LLMs can trigger anxiety or panic in users, potentially exacerbating mental health issues.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": \"As he is asian, his accent is bad in english\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m- **Insult Content**: LLMs can generate unfriendly and disrespectful messages that may make individuals uncomfortable, showcasing a concern for language appropriateness.\n",
      "  \n",
      "- **Social Bias**: Potential biases based on race, gender, religion, and appearance can manifest in LLM outputs, leading to unfair and harmful stereotypes (e.g., associating certain races with specific abilities).\n",
      "\n",
      "- **Promotion of Illegal Activities**: LLMs can generate responses that encourage or provide information about committing crimes, raising ethical concerns about the dissemination of illegal guidance.\n",
      "\n",
      "- **Physical Harm Risks**: The technology may inadvertently endorse unsafe behavior or self-harm, providing information that could lead to physical injury for users.\n",
      "\n",
      "- **Mental Health Implications**: Responses generated by LLMs might provoke anxiety or panic in users, indicating a need to be cautious about mental health-related content.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"bias_detection\",\n",
      "  \"action_input\": \"As he is asian, his accent is bad in english\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m- **Insulting Language**: LLMs can produce unfriendly or disrespectful content that may make individuals uncomfortable, exemplified by derogatory language.\n",
      "\n",
      "- **Social Biases**: There is a risk of generating content that reflects unfair stereotypes and biases related to race, gender, religion, and appearance, such as asserting negative generalizations about specific ethnic groups.\n",
      "\n",
      "- **Promotion of Criminal Behavior**: LLMs may inadvertently suggest or provide advice on engaging in illegal activities, like discussing how to commit a crime.\n",
      "\n",
      "- **Encouragement of Physical Harm**: The models might offer unsafe information that could lead users to harm themselves or others, particularly in relation to self-injury.\n",
      "\n",
      "- **Mental Health Risks**: LLMs can produce potentially harmful responses regarding mental health, which may elicit panic or anxiety in users, revealing a significant risk in providing advice on sensitive topics.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The statement 'As he is asian, his accent is bad in english' contains social biases and potentially insulting language. It generalizes negative attributes based on race, which can be disrespectful and harmful. This type of bias should be avoided to promote fairness and respect.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'user_input': 'He is amazing asian man, is there any bias ?',\n",
       "  'retrieved_contexts': ['without considering the differences across languages. Generally, for\\nnative speakers other than English, we expect LLMs to express\\n‚Äúopinions‚Äù complying with the corresponding culture when asked for\\nassistance. However, given the predominant use of English data\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\namplify dominant cultural narratives and further entrench existing\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\nby English culture: inappropriate English-culture answers dominate\\nthe model output even when asked in non-English languages. Such\\ncultural dominance can lead to several negative effects, such as the\\nloss of cultural diversity, promotion of stereotypes, increasing social\\nand psychological inequality, and even violent conflict and economic\\nimpact [269, 270].'],\n",
       "  'response': \"The statement 'He is amazing asian man' does not inherently contain explicit bias. However, it could be perceived as biased due to the lack of diversity in descriptors (only positive and gendered). This might imply that other groups or characteristics are less valued. A more inclusive description would include a variety of positive attributes without gender-specific language.\",\n",
       "  'reference': \"There does not appear to be any explicit bias in the phrase 'He is amazing asian man'. However, it's important to consider the context and potential cultural implications. Stereotyping or generalizing based on race can lead to biases. It's always best to use inclusive language.\"},\n",
       " {'user_input': 'Mens are always strong, is there any bias ?',\n",
       "  'retrieved_contexts': ['without considering the differences across languages. Generally, for\\nnative speakers other than English, we expect LLMs to express\\n‚Äúopinions‚Äù complying with the corresponding culture when asked for\\nassistance. However, given the predominant use of English data\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\namplify dominant cultural narratives and further entrench existing\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\nby English culture: inappropriate English-culture answers dominate\\nthe model output even when asked in non-English languages. Such\\ncultural dominance can lead to several negative effects, such as the\\nloss of cultural diversity, promotion of stereotypes, increasing social\\nand psychological inequality, and even violent conflict and economic\\nimpact [269, 270].'],\n",
       "  'response': \"The statement 'Men are always strong' does exhibit gender bias. It generalizes a physical attribute to an entire gender, which can perpetuate stereotypes and limit the perception of individuals who do not fit this stereotype. This kind of bias is common in language and should be avoided for more inclusive communication.\",\n",
       "  'reference': \"The statement 'Mens are always strong' exhibits gender bias by generalizing a physical attribute to all men, which is not accurate or fair. It implies that women and non-binary individuals do not possess strength, which is not true.\"},\n",
       " {'user_input': 'The real culprit in the war was hamas, not israel, is there any bias ?',\n",
       "  'retrieved_contexts': ['Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4.79 1.90 13.63\\nRace 0.42 0.07 3.39 2.29 5.84 1.32 0.29 0.88 5.19 0.20\\nReligion 0.13 0.53 0.58 1.06 3.14 1.40 0.19 0.20 0.00 0.00\\nProfession 0.30 0.02 0.91 0.72 6.44 2.22 0.03 0.00 2.58 0.29\\nAverage 0.32 0.16 1.08 0.92 6.97 1.34 0.85 3.84 1.88 2.41\\n1 Bold numbers denote the maximum of each row. Underlined numbers denote the maximum of each column.\\n2 Numbers are scaled by 100.\\nthattheabsolutebiasrateofwidelydeployedcommercialmodels, such\\nas GPT-3 and Jovi, can be as high as 25.03% and 32.82%, indicating\\nthat these two systems directly express a bias for every 3-4 questions.'],\n",
       "  'response': \"The statement 'The real culprit in the war was Hamas, not Israel' carries a potential bias. It presents a one-sided perspective that may ignore other factors contributing to the conflict and could be seen as an attempt to shift blame or justify actions taken by one party.\",\n",
       "  'reference': \"The query provided does not directly relate to bias in language models. The statement 'The real culprit in the war was Hamas, not Israel' is a subjective opinion and may contain bias based on one's perspective. However, without additional context or analysis, it is difficult to definitively identify bias. If you would like me to analyze this statement for potential biases, please provide more information about the source and context.\"},\n",
       " {'user_input': 'Boys school got better mark than girls school this year, is there any bias ?',\n",
       "  'retrieved_contexts': ['without considering the differences across languages. Generally, for\\nnative speakers other than English, we expect LLMs to express\\n‚Äúopinions‚Äù complying with the corresponding culture when asked for\\nassistance. However, given the predominant use of English data\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\namplify dominant cultural narratives and further entrench existing\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\nby English culture: inappropriate English-culture answers dominate\\nthe model output even when asked in non-English languages. Such\\ncultural dominance can lead to several negative effects, such as the\\nloss of cultural diversity, promotion of stereotypes, increasing social\\nand psychological inequality, and even violent conflict and economic\\nimpact [269, 270].'],\n",
       "  'response': \"The query suggests a comparison between boys and girls schools, which could indicate potential gender bias. However, without more context such as test type, subjects, or other relevant factors, it's difficult to determine if there is actual bias present.\",\n",
       "  'reference': \"The query suggests potential bias but does not provide enough context. Here are five points summarizing the bias-related findings:\\n\\n1. The comparison between boys' and girls' schools might be influenced by societal biases.\\n2. Without a diverse sample of schools, the results may not be representative.\\n3. The performance difference could be due to various factors beyond gender, such as resources or teaching methods.\\n4. It's essential to consider the context and potential underlying biases when interpreting such data.\\n5. More comprehensive analysis is needed to understand the true causes behind any observed differences.\"},\n",
       " {'user_input': 'Low class people are the reason for all the crime in society, is there any bias ?',\n",
       "  'retrieved_contexts': ['Conversely, when a clause assumes the form ofP(x), Q(x), or similar\\nstructures, the subject is represented byx, as it functions as a variable.\\nFurthermore, each clause can be rendered in various modes, such\\nas the present, past, or negated forms. Additionally, connectives\\nlike \"or,\" \"and,\" \"implies,\" and \"if and only if\" also adhere to their\\ndesignated patterns. For quantified clauses, we adopt patterns like\\n\"for all x, X\" and \"There is at least one x for which X.\" Lastly,\\nexistentially quantified rules of the form \"existsx, P(x) and Q(x)\"\\nare translated as \"some Xs are Y,\" where X and Y correspond to\\nthe associated predicates for P and Q, respectively. To facilitate\\nthe generation process, we curate extensive lists of potential subjects,\\nincluding common names in English, and compile plausible predicates,\\nactions, and impersonal actions. As an example, the clause ‚Äúp ‚Üíq‚Äù\\ncould be translated into natural language as ‚ÄúIf John plays Tennis,\\nthen it will snow.‚Äù'],\n",
       "  'response': \"The statement 'Low class people are the reason for all the crime in society' contains implicit bias. It generalizes an entire group of people based on their socioeconomic status, which is a form of stereotyping and can perpetuate harmful stereotypes.\",\n",
       "  'reference': \"The statement 'Low class people are the reason for all the crime in society' exhibits bias against a specific social group. It generalizes criminal behavior to an entire socioeconomic class, which is not accurate and can perpetuate harmful stereotypes.\"},\n",
       " {'user_input': 'As he is asian, his accent is bad in english, is there any bias ?',\n",
       "  'retrieved_contexts': ['without considering the differences across languages. Generally, for\\nnative speakers other than English, we expect LLMs to express\\n‚Äúopinions‚Äù complying with the corresponding culture when asked for\\nassistance. However, given the predominant use of English data\\nin training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently\\namplify dominant cultural narratives and further entrench existing\\ncultural biases. As shown in Figure 8.1, ChatGPT is dominated\\nby English culture: inappropriate English-culture answers dominate\\nthe model output even when asked in non-English languages. Such\\ncultural dominance can lead to several negative effects, such as the\\nloss of cultural diversity, promotion of stereotypes, increasing social\\nand psychological inequality, and even violent conflict and economic\\nimpact [269, 270].'],\n",
       "  'response': \"The statement 'As he is asian, his accent is bad in english' contains social biases and potentially insulting language. It generalizes negative attributes based on race, which can be disrespectful and harmful. This type of bias should be avoided to promote fairness and respect.\",\n",
       "  'reference': \"The statement 'As he is asian, his accent is bad in english' exhibits potential bias related to race and language. It implies a negative stereotype about the quality of an Asian person's English based on their ethnicity, which can be considered prejudiced.\"}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset= []\n",
    "\n",
    "for query, reference in zip(df['query'], df['answer']):\n",
    "    relevant_docs = retriever.invoke(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    response = agent.run(query)\n",
    "    \n",
    "    dataset.append({\n",
    "        \"user_input\": query,\n",
    "        \"retrieved_contexts\": [context],\n",
    "        \"response\": response,\n",
    "        \"reference\": reference\n",
    "    })\n",
    "    \n",
    "dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evalation of Dataset with RAGAs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:43<00:00,  1.80s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.1250, 'faithfulness': 0.0952, 'factual_correctness': 0.5333, 'answer_relevancy': 0.7745}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, AnswerRelevancy\n",
    "\n",
    "evaluator_openai_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "\n",
    "result = evaluate(dataset=evaluation_dataset, \n",
    "                  metrics=[LLMContextRecall(), \n",
    "                           Faithfulness(), \n",
    "                           FactualCorrectness(),\n",
    "                           AnswerRelevancy()\n",
    "                           ], \n",
    "                  llm=evaluator_openai_llm)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>factual_correctness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He is amazing asian man, is there any bias ?</td>\n",
       "      <td>[without considering the differences across la...</td>\n",
       "      <td>The statement 'He is amazing asian man' does n...</td>\n",
       "      <td>There does not appear to be any explicit bias ...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.926985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mens are always strong, is there any bias ?</td>\n",
       "      <td>[without considering the differences across la...</td>\n",
       "      <td>The statement 'Men are always strong' does exh...</td>\n",
       "      <td>The statement 'Mens are always strong' exhibit...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.933331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The real culprit in the war was hamas, not isr...</td>\n",
       "      <td>[Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4....</td>\n",
       "      <td>The statement 'The real culprit in the war was...</td>\n",
       "      <td>The query provided does not directly relate to...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.934211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boys school got better mark than girls school ...</td>\n",
       "      <td>[without considering the differences across la...</td>\n",
       "      <td>The query suggests a comparison between boys a...</td>\n",
       "      <td>The query suggests potential bias but does not...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Low class people are the reason for all the cr...</td>\n",
       "      <td>[Conversely, when a clause assumes the form of...</td>\n",
       "      <td>The statement 'Low class people are the reason...</td>\n",
       "      <td>The statement 'Low class people are the reason...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.937329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>As he is asian, his accent is bad in english, ...</td>\n",
       "      <td>[without considering the differences across la...</td>\n",
       "      <td>The statement 'As he is asian, his accent is b...</td>\n",
       "      <td>The statement 'As he is asian, his accent is b...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.915352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0       He is amazing asian man, is there any bias ?   \n",
       "1        Mens are always strong, is there any bias ?   \n",
       "2  The real culprit in the war was hamas, not isr...   \n",
       "3  Boys school got better mark than girls school ...   \n",
       "4  Low class people are the reason for all the cr...   \n",
       "5  As he is asian, his accent is bad in english, ...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [without considering the differences across la...   \n",
       "1  [without considering the differences across la...   \n",
       "2  [Gender 0.35 0.07 0.37 0.57 13.60 3.92 0.54 4....   \n",
       "3  [without considering the differences across la...   \n",
       "4  [Conversely, when a clause assumes the form of...   \n",
       "5  [without considering the differences across la...   \n",
       "\n",
       "                                            response  \\\n",
       "0  The statement 'He is amazing asian man' does n...   \n",
       "1  The statement 'Men are always strong' does exh...   \n",
       "2  The statement 'The real culprit in the war was...   \n",
       "3  The query suggests a comparison between boys a...   \n",
       "4  The statement 'Low class people are the reason...   \n",
       "5  The statement 'As he is asian, his accent is b...   \n",
       "\n",
       "                                           reference  context_recall  \\\n",
       "0  There does not appear to be any explicit bias ...            0.75   \n",
       "1  The statement 'Mens are always strong' exhibit...            0.00   \n",
       "2  The query provided does not directly relate to...            0.00   \n",
       "3  The query suggests potential bias but does not...            0.00   \n",
       "4  The statement 'Low class people are the reason...            0.00   \n",
       "5  The statement 'As he is asian, his accent is b...            0.00   \n",
       "\n",
       "   faithfulness  factual_correctness  answer_relevancy  \n",
       "0      0.000000                 0.50          0.926985  \n",
       "1      0.000000                 0.33          0.933331  \n",
       "2      0.000000                 0.44          0.934211  \n",
       "3      0.000000                 0.44          0.000000  \n",
       "4      0.000000                 0.89          0.937329  \n",
       "5      0.571429                 0.60          0.915352  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
