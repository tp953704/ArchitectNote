{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test AI Agent ðŸ¤– Tool Calling with DeepEval ðŸ§ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "load = load_dotenv('./../.env')\n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model = \"qwen2.5:latest\",\n",
    "    temperature=0.5,\n",
    "    max_tokens = 250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n",
       "</pre>\n"
      ],
      "text/plain": [
       "ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deepeval\n",
    "\n",
    "deepeval.login_with_confident_api_key(\"+BNkBlukxi5blix8dBBhkK3jUhXswK355A/tIjHjQuk=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the tools to pass in our AI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"Add two numbers and return results.\"\n",
    "    return int(a) + int(b)\n",
    "\n",
    "@tool\n",
    "def subtract_numbers(a: int, b: int) -> int:\n",
    "    \"Subtract two numbers and return results.\"\n",
    "    return int(a) - int(b)\n",
    "\n",
    "@tool\n",
    "def multiply_numbers(a: int, b: int) -> int:\n",
    "    \"Multiply two numbers and return results.\"\n",
    "    return int(a) * int(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add_numbers, subtract_numbers, multiply_numbers]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Agent code to invoke our custom tooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"multiply_numbers\",\n",
      "  \"action_input\": {\n",
      "    \"a\": 2,\n",
      "    \"b\": 2\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m4\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The double of 2 is 4\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'What is the double of 2', 'output': 'The double of 2 is 4', 'intermediate_steps': [(AgentAction(tool='multiply_numbers', tool_input={'a': 2, 'b': 2}, log='Action:\\n```\\n{\\n  \"action\": \"multiply_numbers\",\\n  \"action_input\": {\\n    \"a\": 2,\\n    \"b\": 2\\n  }\\n}\\n```'), 4)]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools= tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True\n",
    ")\n",
    "\n",
    "query = \"What is the double of 2\"\n",
    "\n",
    "result = agent.invoke(query)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('multiply_numbers', {'a': 2, 'b': 2}, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_action, results = result['intermediate_steps'][0]\n",
    "tool_output = result['intermediate_steps'][0][1]\n",
    "\n",
    "tool = agent_action.tool\n",
    "tool_input = agent_action.tool_input\n",
    "\n",
    "tool, tool_input, tool_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Golden Dataset for DeepEval Testcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the sum of 2 and 4',\n",
       "  'expected_answer': 'The sum of 2 and 4 is 6',\n",
       "  'tool_called': [ToolCall(\n",
       "       name=\"add_numbers\",\n",
       "       input_parameters={\n",
       "           \"a\": 2,\n",
       "           \"b\": 4\n",
       "       }\n",
       "   )]},\n",
       " {'question': 'What is double of 2?',\n",
       "  'expected_answer': 'The double of 2 is 4.',\n",
       "  'tool_called': [ToolCall(\n",
       "       name=\"multiply_numbers\",\n",
       "       input_parameters={\n",
       "           \"a\": 2,\n",
       "           \"b\": 4\n",
       "       }\n",
       "   )]},\n",
       " {'question': 'What is 2 minus 4?',\n",
       "  'expected_answer': '2 minus 4 is -2.',\n",
       "  'tool_called': [ToolCall(\n",
       "       name=\"subtract_numbers\",\n",
       "       input_parameters={\n",
       "           \"a\": 2,\n",
       "           \"b\": 4\n",
       "       }\n",
       "   )]}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.test_case import ToolCall\n",
    "\n",
    "golden_dataset = [\n",
    "    \n",
    "    {\n",
    "        \"question\": \"What is the sum of 2 and 4\",\n",
    "        \"expected_answer\": \"The sum of 2 and 4 is 6\",\n",
    "        \"tool_called\": [\n",
    "            ToolCall(name=\"add_numbers\", input_parameters={\"a\": 2, \"b\": 4})\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is double of 2?\",\n",
    "        \"expected_answer\": \"The double of 2 is 4.\",\n",
    "        \"tool_called\": [\n",
    "            ToolCall(name=\"multiply_numbers\", input_parameters={\"a\": 2, \"b\": 4})\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is 2 minus 4?\",\n",
    "        \"expected_answer\": \"2 minus 4 is -2.\",\n",
    "        \"tool_called\": [\n",
    "            ToolCall(name=\"subtract_numbers\", input_parameters={\"a\": 2, \"b\": 4})\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "]\n",
    "\n",
    "golden_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the Golden Dataset to DeepEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import EvaluationDataset, Golden\n",
    "\n",
    "goldens = []\n",
    "\n",
    "for data in golden_dataset:\n",
    "    golden = Golden(\n",
    "        input=data['question'],\n",
    "        expected_output=data['expected_answer'],\n",
    "        tools_called=data['tool_called']\n",
    "    )\n",
    "    goldens.append(golden)\n",
    "    \n",
    "dataset = EvaluationDataset(goldens=goldens)\n",
    "\n",
    "dataset.push(\"TestingToolCalls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(test_cases=[], goldens=[Golden(input='What is the sum of 2 and 4', actual_output=None, expected_output='The sum of 2 and 4 is 6', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=[ToolCall(\n",
       "    name=\"add_numbers\",\n",
       "    input_parameters={\n",
       "        \"a\": 2,\n",
       "        \"b\": 4\n",
       "    }\n",
       ")], expected_tools=None, source_file=None), Golden(input='What is double of 2?', actual_output=None, expected_output='The double of 2 is 4.', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=[ToolCall(\n",
       "    name=\"multiply_numbers\",\n",
       "    input_parameters={\n",
       "        \"a\": 2,\n",
       "        \"b\": 4\n",
       "    }\n",
       ")], expected_tools=None, source_file=None), Golden(input='What is 2 minus 4?', actual_output=None, expected_output='2 minus 4 is -2.', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=[ToolCall(\n",
       "    name=\"subtract_numbers\",\n",
       "    input_parameters={\n",
       "        \"a\": 2,\n",
       "        \"b\": 4\n",
       "    }\n",
       ")], expected_tools=None, source_file=None)], conversational_goldens=[], _alias=None, _id=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.pull(\"TestingToolCalls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(test_cases=[LLMTestCase(input='What is the sum of 2 and 4', actual_output=None, expected_output='The sum of 2 and 4 is 6', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=[ToolCall(\n",
       "    name=\"add_numbers\",\n",
       "    input_parameters={\n",
       "        \"a\": 2,\n",
       "        \"b\": 4\n",
       "    }\n",
       ")], expected_tools=None, reasoning=None, name=None), LLMTestCase(input='What is double of 2?', actual_output=None, expected_output='The double of 2 is 4.', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=[ToolCall(\n",
       "    name=\"multiply_numbers\",\n",
       "    input_parameters={\n",
       "        \"a\": 2,\n",
       "        \"b\": 4\n",
       "    }\n",
       ")], expected_tools=None, reasoning=None, name=None), LLMTestCase(input='What is 2 minus 4?', actual_output=None, expected_output='2 minus 4 is -2.', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=[ToolCall(\n",
       "    name=\"subtract_numbers\",\n",
       "    input_parameters={\n",
       "        \"a\": 2,\n",
       "        \"b\": 4\n",
       "    }\n",
       ")], expected_tools=None, reasoning=None, name=None)], goldens=[], conversational_goldens=[], _alias=TestingToolCalls, _id=cm7qkyyjx0letz2hur3wgv2lz)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating LLMTestCase and invoking the AI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ai_agent(question):\n",
    "    response = agent.invoke(question)\n",
    "    intermediate_steps = response['intermediate_steps']\n",
    "    agent_action, results = intermediate_steps[0]\n",
    "    tool_output = intermediate_steps[0][1]\n",
    "    tool = agent_action.tool\n",
    "    tool_input = agent_action.tool_input\n",
    "    return response,tool, tool_input, tool_output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Test Case for DeepEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"add_numbers\",\n",
      "  \"action_input\": {\n",
      "    \"a\": 2,\n",
      "    \"b\": 4\n",
      "  }\n",
      "}\n",
      "```~\n",
      "<|im_start|>\n",
      "<|im_start|>user\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m6\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI know what to respond\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The sum of 2 and 4 is 6.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To find double of 2, I can simply multiply 2 by 2.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"multiply_numbers\",\n",
      "  \"action_input\": {\n",
      "    \"a\": 2,\n",
      "    \"b\": 2\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m4\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI know what to respond\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"4\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "```\n",
      "{\n",
      "  \"action\": \"subtract_numbers\",\n",
      "  \"action_input\": {\n",
      "    \"a\": 2,\n",
      "    \"b\": 4\n",
      "  }\n",
      "}\n",
      "``` `\n",
      "<|im_start|>\n",
      "<|im_start|>user\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m-2\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI know what to respond\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"-2\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from deepeval.dataset import Golden\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from typing import List\n",
    "\n",
    "def convert_goldens_to_test_cases(goldens: List[Golden]) -> List[LLMTestCase]:\n",
    "    test_cases = []\n",
    "    for golden in goldens:\n",
    "        response,tool,tool_input, tool_output = query_ai_agent(golden.input)\n",
    "        test_case = LLMTestCase(\n",
    "            input=golden.input,\n",
    "            actual_output=response,\n",
    "            tools_called=[ToolCall(name=tool, input_parameters=tool_input, output=tool_output)],\n",
    "            expected_tools=golden.tools_called\n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "    return test_cases\n",
    "\n",
    "data = convert_goldens_to_test_cases(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LLMTestCase(input='What is the sum of 2 and 4', actual_output={'input': 'What is the sum of 2 and 4', 'output': 'The sum of 2 and 4 is 6.', 'intermediate_steps': [(AgentAction(tool='add_numbers', tool_input={'a': 2, 'b': 4}, log='Action:\\n```\\n{\\n  \"action\": \"add_numbers\",\\n  \"action_input\": {\\n    \"a\": 2,\\n    \"b\": 4\\n  }\\n}\\n```~\\n<|im_start|>\\n<|im_start|>user\\n'), 6)]}, expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=[ToolCall(\n",
       "     name=\"add_numbers\",\n",
       "     input_parameters={\n",
       "         \"a\": 2,\n",
       "         \"b\": 4\n",
       "     },\n",
       "     output=6\n",
       " )], expected_tools=[ToolCall(\n",
       "     name=\"add_numbers\",\n",
       "     input_parameters={\n",
       "         \"a\": 2,\n",
       "         \"b\": 4\n",
       "     }\n",
       " )], reasoning=None, name=None),\n",
       " LLMTestCase(input='What is double of 2?', actual_output={'input': 'What is double of 2?', 'output': '4', 'intermediate_steps': [(AgentAction(tool='multiply_numbers', tool_input={'a': 2, 'b': 2}, log='Thought: To find double of 2, I can simply multiply 2 by 2.\\n\\nAction:\\n```\\n{\\n  \"action\": \"multiply_numbers\",\\n  \"action_input\": {\\n    \"a\": 2,\\n    \"b\": 2\\n  }\\n}\\n```'), 4)]}, expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=[ToolCall(\n",
       "     name=\"multiply_numbers\",\n",
       "     input_parameters={\n",
       "         \"a\": 2,\n",
       "         \"b\": 2\n",
       "     },\n",
       "     output=4\n",
       " )], expected_tools=[ToolCall(\n",
       "     name=\"multiply_numbers\",\n",
       "     input_parameters={\n",
       "         \"a\": 2,\n",
       "         \"b\": 4\n",
       "     }\n",
       " )], reasoning=None, name=None),\n",
       " LLMTestCase(input='What is 2 minus 4?', actual_output={'input': 'What is 2 minus 4?', 'output': '-2', 'intermediate_steps': [(AgentAction(tool='subtract_numbers', tool_input={'a': 2, 'b': 4}, log='Action:\\n```\\n{\\n  \"action\": \"subtract_numbers\",\\n  \"action_input\": {\\n    \"a\": 2,\\n    \"b\": 4\\n  }\\n}\\n``` `\\n<|im_start|>\\n<|im_start|>user\\n'), -2)]}, expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=[ToolCall(\n",
       "     name=\"subtract_numbers\",\n",
       "     input_parameters={\n",
       "         \"a\": 2,\n",
       "         \"b\": 4\n",
       "     },\n",
       "     output=-2\n",
       " )], expected_tools=[ToolCall(\n",
       "     name=\"subtract_numbers\",\n",
       "     input_parameters={\n",
       "         \"a\": 2,\n",
       "         \"b\": 4\n",
       "     }\n",
       " )], reasoning=None, name=None)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ToolCorrectionMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/karthik/tryout/LangchainTraining/myenv312/lib/python3.12/site-packages/rich/live.py:231: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/karthik/tryout/LangchainTraining/myenv312/lib/python3.12/site-packages/rich/live.py:231: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "All expected tools ['add_numbers'] were called (order not considered).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "All expected tools ['multiply_numbers'] were called (order not considered).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "All expected tools ['subtract_numbers'] were called (order not considered).\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ToolCorrectnessMetric\n",
    "from deepeval.test_case import LLMTestCase, ToolCall\n",
    "\n",
    "\n",
    "for testcase in data:\n",
    "    metric = ToolCorrectnessMetric()\n",
    "    metric.measure(testcase)\n",
    "    print(metric.score)\n",
    "    print(metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digging into ToolCorrectionMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "All expected tools ['subtract_numbers'] were called (order not considered).\n",
      "True\n",
      "[ToolCall(\n",
      "    name=\"subtract_numbers\",\n",
      "    input_parameters={\n",
      "        \"a\": 2,\n",
      "        \"b\": 4\n",
      "    }\n",
      ")]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "All expected tools ['subtract_numbers'] were called (order not considered).\n",
      "True\n",
      "[ToolCall(\n",
      "    name=\"subtract_numbers\",\n",
      "    input_parameters={\n",
      "        \"a\": 2,\n",
      "        \"b\": 4\n",
      "    }\n",
      ")]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "All expected tools ['subtract_numbers'] were called (order not considered).\n",
      "True\n",
      "[ToolCall(\n",
      "    name=\"subtract_numbers\",\n",
      "    input_parameters={\n",
      "        \"a\": 2,\n",
      "        \"b\": 4\n",
      "    }\n",
      ")]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ToolCorrectnessMetric\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "\n",
    "tool_metrics = ToolCorrectnessMetric(\n",
    "    threshold=1,\n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.INPUT,\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "        LLMTestCaseParams.EXPECTED_OUTPUT,\n",
    "        LLMTestCaseParams.EXPECTED_TOOLS\n",
    "    ],\n",
    "    include_reason=True,\n",
    "    should_exact_match=True,\n",
    "    should_consider_ordering=True,\n",
    "    strict_mode=True    \n",
    ")\n",
    "\n",
    "\n",
    "for testcase in data:\n",
    "    tool_metrics.measure(testcase)\n",
    "    print(metric.score)\n",
    "    print(metric.reason)\n",
    "    print(metric.include_reason)\n",
    "    print(metric.expected_tools)\n",
    "    print(metric.evaluation_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
