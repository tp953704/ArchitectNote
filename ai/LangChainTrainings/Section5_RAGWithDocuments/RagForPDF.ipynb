{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG with PDF üìÑ Data extraction to give context to LLM üß†\n",
    "\n",
    "<img src=\"./Images/RAGs.png\" width=\"800\" height=\"700\" style=\"display: block; margin: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Using cached pypdf-5.3.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Using cached pypdf-5.3.0-py3-none-any.whl (300 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load = load_dotenv('./../.env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model = \"qwen2.5:latest\",\n",
    "    temperature=0.5,\n",
    "    max_tokens = 250\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extracting the PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf1 = \"./attention.pdf\"\n",
    "pdf2 = \"./LLMForgetting.pdf\"\n",
    "pdf3 = \"./TestingAndEvaluatingLLM.pdf\"\n",
    "\n",
    "pdfFiles = [pdf1, pdf2, pdf3]\n",
    "\n",
    "documents = []\n",
    "\n",
    "for pdf in pdfFiles:\n",
    "    loader = PyPDFLoader(pdf)\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': './attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani‚àó\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer‚àó\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar‚àó\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit‚àó\\nGoogle Research\\nusz@google.com\\nLlion Jones‚àó\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez‚àó ‚Ä†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n≈Åukasz Kaiser‚àó\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin‚àó ‚Ä°\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n‚Ä†Work performed while at Google Brain.\\n‚Ä°Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023')]\n"
     ]
    }
   ],
   "source": [
    "print(documents[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Splitting into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, \n",
    "                                               chunk_overlap=200, add_start_index=True)\n",
    "\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 3072)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.2:latest\")\n",
    "\n",
    "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
    "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
    "\n",
    "assert len(vector_1) == len(vector_2)\n",
    "len(vector_1), len(vector_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install -qU langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents = all_splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Retriving from the Persistant Vector Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 CHAPTER 4. LOGICAL REASONING CORRECTNESS\n",
      "the following challenges: 1) If an LLM concludes correctly, it is unclear\n",
      "whether the response stems from reasoning or merely relies on simple\n",
      "heuristics such as memorization or word correlations (e.g., ‚Äúdry floor‚Äù\n",
      "is more likely to correlate with ‚Äúplaying football‚Äù). 2) If an LLM\n",
      "fails to reason correctly, it is not clear which part of the reasoning\n",
      "process it failed (i.e., inferring not raining from floor being dry or\n",
      "inferring playing football from not raining). 3) There is a lack of\n",
      "a system that can organize such test cases to cover all other formal\n",
      "reasoning scenarios besides implication, such as logical equivalence\n",
      "(e.g., If A then B, if B then A; therefore, A if and only if B). 4)\n",
      "Furthermore, understanding an LLM‚Äôs performance on such test cases\n",
      "provides little guidance on improving the reasoning ability of the\n",
      "LLM. To better handle these challenges, a well-performing testing\n",
      "Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang,\n",
      "‚ÄúLarge language models for software engineering: A systematic\n",
      "literature review,‚ÄùarXiv, 2024.\n",
      "[87] Yuxuan Wan, Chaozheng Wang, Yi Dong, Wenxuan Wang,\n",
      "Shuqing Li, Yintong Huo, and Michael R Lyu, ‚ÄúAutomatically\n",
      "generating ui code from screenshot: A divide-and-conquer-based\n",
      "approach,‚Äù arXiv preprint arXiv:2406.16386, 2024.\n",
      "[88] Chaozheng Wang, Zongjie Li, Cuiyun Gao, Wenxuan Wang,\n",
      "Ting Peng, Hailiang Huang, Yuetang Deng, Shuai Wang, and\n",
      "Michael R Lyu, ‚ÄúExploring multi-lingual bias of large code\n",
      "models in code generation,‚Äù arXiv preprint arXiv:2404.19368,\n",
      "2024.\n",
      "[89] Yun Peng, Chaozheng Wang, Wenxuan Wang, Cuiyun Gao, and\n",
      "Michael R. Lyu, ‚ÄúGenerative type inference for python,‚ÄùASE,\n",
      "2023.\n",
      "[90] Yun Peng, Shuqing Li, Wenwei Gu, Yichen Li, Wenxuan Wang,\n",
      "Cuiyun Gao, and Michael R Lyu, ‚ÄúRevisiting, benchmarking\n",
      "prompting method to improve LLM‚Äôs multilingual safety by enhancing\n",
      "cross-lingual generalization of safety alignment.\n",
      "Third, to evaluate the fairness of LLMs, I introduce two evaluation\n",
      "frameworks, BiasAsker and XCulturalBench, to measure the social\n",
      "bias and cultural bias of LLMs, respectively. I first introduce\n",
      "BiasAsker, an automated framework to identify and measure social\n",
      "bias in conversational AI systems. BiasAsker can measure the bias\n",
      "altitudes on 841 groups from 5,021 biased properties perspective by\n",
      "asking various kinds of questions. Experiments on 10 commercial sys-\n",
      "tems and models show the effectiveness of BiasAsker. Then, I identify\n",
      "a cultural dominance issue within LLMs due to the predominant use\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "vector_store = Chroma(persist_directory='./chroma_langchain_db', embedding_function=embeddings)\n",
    "\n",
    "result = vector_store.similarity_search(\"What is Bias testing\", k=3)\n",
    "\n",
    "for doc in result:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(id='16eeba20-c1be-42a2-8243-a14c71563837', metadata={'author': '', 'creationdate': '2025-01-07T01:36:50+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-07T01:36:50+00:00', 'page': 9, 'page_label': '10', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': './LLMForgetting.pdf', 'start_index': 0, 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Under review\\nFigure 6: The performance of general knowledge of the BLOOMZ-7.1b and LLAMA-7b\\nmodel trained on the instruction data and the mixed data. The dashed lines refers to the\\nperformance of BLOOMZ-7.1b and LLAMA-7B and the solid ones refer to those of mixed-\\ninstruction trained models.\\nincreases to 3b, BLOOMZ-3b suffers less forgetting compared to mT0-3.7B. For example, the\\nFG value of BLOOMZ-3b is 11.09 which is 5.64 lower than that of mT0-3.7b. These results\\nsuggest that BLOOMZ, which has a decoder-only model architecture, can maintain more\\nknowledge during continual instruction tuning. This difference may be attributed to the\\nautoregressive nature of the model or the differences in training objectives. Furthermore,\\nthe results imply that as the model scale increases, decoder-only models may suffer from\\nless catastrophic forgetting compared to encoder-decoder models. As we observe, the\\nknowledge degraded more drastically in mT0.\\n5.4 Effect of General Instruction Tuning'),\n",
       " 0.8988205790519714)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = vector_store.similarity_search_with_score(\"What are the types of LLM Testing\")\n",
    "\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Retrivers in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='16eeba20-c1be-42a2-8243-a14c71563837', metadata={'author': '', 'creationdate': '2025-01-07T01:36:50+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-07T01:36:50+00:00', 'page': 9, 'page_label': '10', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': './LLMForgetting.pdf', 'start_index': 0, 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='Under review\\nFigure 6: The performance of general knowledge of the BLOOMZ-7.1b and LLAMA-7b\\nmodel trained on the instruction data and the mixed data. The dashed lines refers to the\\nperformance of BLOOMZ-7.1b and LLAMA-7B and the solid ones refer to those of mixed-\\ninstruction trained models.\\nincreases to 3b, BLOOMZ-3b suffers less forgetting compared to mT0-3.7B. For example, the\\nFG value of BLOOMZ-3b is 11.09 which is 5.64 lower than that of mT0-3.7b. These results\\nsuggest that BLOOMZ, which has a decoder-only model architecture, can maintain more\\nknowledge during continual instruction tuning. This difference may be attributed to the\\nautoregressive nature of the model or the differences in training objectives. Furthermore,\\nthe results imply that as the model scale increases, decoder-only models may suffer from\\nless catastrophic forgetting compared to encoder-decoder models. As we observe, the\\nknowledge degraded more drastically in mT0.\\n5.4 Effect of General Instruction Tuning')],\n",
       " [Document(id='91223f07-2566-47ff-84df-98a1c0d67eaf', metadata={'author': '', 'creationdate': '2024-09-04T00:37:21+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-09-04T00:37:21+00:00', 'page': 76, 'page_label': '58', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': './TestingAndEvaluatingLLM.pdf', 'start_index': 772, 'subject': '', 'title': '', 'total_pages': 223, 'trapped': '/False'}, page_content='sive performance[89]. Some other works leverage LLMs‚Äô reasoning\\nability to repair programs automatically by providing semantically\\nsimilar bug fixes and reasoning hints[162]. However, many recent\\nstudies question the actual reasoning capacity of LLMs. For example,\\nrecent research by Google DeepMind [163] argues that LLMs cannot\\nself-correct reasoning, and another study suggests that LLMs are still\\nstruggling to address newly-created datasets despite their astonishing\\nperformance on well-known benchmark datasets [164].\\nLLMs with unreliable reasoning ability could induce severe conse-\\nquences in the real world. First, their problem-solving capabilities\\ncan be significantly impeded and it undermines the credibility of\\nmany downstream research and tools [89, 162, 165]. Second, it may\\npotentially generate inaccurate or misleading information, leading\\nusers to make uninformed decisions or develop misconceptions based\\non the flawed output from the LLMs. For example, LLaMA 2')],\n",
       " [Document(id='2fe0ed77-b5ac-4b93-9f69-773a290e7061', metadata={'author': '', 'creationdate': '2024-09-04T00:37:21+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-09-04T00:37:21+00:00', 'page': 102, 'page_label': '84', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': './TestingAndEvaluatingLLM.pdf', 'start_index': 1552, 'subject': '', 'title': '', 'total_pages': 223, 'trapped': '/False'}, page_content='noise when reading (e.g., ‚Äú*‚Äù in ‚ÄúH*ell*o‚Äù). Specifically, ‚ÄúHello‚Äù has\\nmultiple ‚Äúo‚Äùs, and ‚Äú*‚Äù is a mathematical symbol outside the English\\nalphabet. Therefore, humans can easily ignore the noises.\\nMR1-5 Character Masking\\nThis MR uses character masking, which masks a small portion')]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs = {\"k\": 1}\n",
    ")\n",
    "\n",
    "retriever.batch(\n",
    "    [\n",
    "        \"What is the Bias Measurement\",\n",
    "        \"How to test human safety against LLM\",\n",
    "        \"How LLM forgets the context\"\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Retrival Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/c7p4f2sd36xbqwkp2djn8flc0000gn/T/ipykernel_80392/2281395069.py:6: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Testing the Factual Correctness of LLMs involves assessing their performance on multi-hop questions. Specifically:**\n",
      "\n",
      "- **Multi-hop Questions**: These are more challenging for LLMs compared to single-hop questions.\n",
      "- **FactChecker Tool**: Used to generate 600 multi-hop questions and evaluate LLM responses.\n",
      "- **Evaluation Outcome**: All LLMs showed a higher incidence of factual errors in responding to 2-hop questions, indicating increased difficulty with complex queries.\n",
      "- **Findings**:\n",
      "  - FactChecker can identify significant factual errors in both commercial and research LLMs.\n",
      "  - It provides an evaluation on the factual accuracy of these models.\n",
      "\n",
      "This testing approach highlights the limitations of current LLMs in handling complex, multi-step reasoning tasks.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "query = \"What exactly does Testing the Factual Correctness of LLM tells\"\n",
    "\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an AI Assisant. Use the following context to answer the question correctly.\n",
    "    If you dont know the answer, just tell, I dont know.\n",
    "    \n",
    "    Also, summarize the response in MD format\n",
    "    \n",
    "    \"context: {context} \\n\\n\"\n",
    "    \"question: {question} \\n\\n\"\n",
    "    \"AI answer:\n",
    "    \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"context\": context_text, \"question\": query})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Langchain Hub for Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To test translation in LLMs like ChatGPT and GPT-4, you can use human evaluations on randomly selected responses where the models have differing judgments. Translate responses from other languages to English using tools like Google Translate before feeding them into the model for evaluation. This helps ensure consistency across language outputs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "\n",
    "query = \"How to test Translation in LLM?\"\n",
    "\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"context\": context_text, \"question\": query})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving data using RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the factual correctness of large language models (LLMs) involves assessing their ability to provide accurate information. This is done by generating multi-hop questions, which are more complex and require the model to connect multiple pieces of information to arrive at a correct answer. The study uses FactChecker to create 600 multi-hop questions and queries various LLMs with these questions. The results show that all tested LLMs make more factual errors when answering multi-hop questions compared to single-hop ones, indicating that multi-hop questions present a greater challenge for the models.\n",
      "\n",
      "This testing process helps identify substantial factual errors in both commercial and research LLMs and provides an evaluation of their factual accuracy. It highlights the need for improved capabilities in handling complex, interconnected information to enhance the reliability of these models.\n",
      "\n",
      "üìï Sources Used:\n",
      "- ./TestingAndEvaluatingLLM.pdf\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever, return_source_documents=True)\n",
    "\n",
    "question = \"What exactly does Testing the Factual Correctness of LLM tells\"\n",
    "\n",
    "response = qa_chain.invoke(question)\n",
    "\n",
    "sources = set(doc.metadata.get(\"source\", \"Unknown\") for doc in response[\"source_documents\"])\n",
    "\n",
    "print(response['result'])\n",
    "print(\"\\nüìï Sources Used:\")\n",
    "for source in sources:\n",
    "    print(f\"- {source}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
