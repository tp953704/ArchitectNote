# lession1
```
***

**Speaker:** Welcome to this section on deploying a retail application on Google Cloud. With Google Cloud you can modernize your retail applications and power your eCommerce operations with reliability, scalability, and flexibility. Deploy retail applications to deliver frictionless customer experiences. In this section you learn to architect a strategy to deploy a retail application on Google Compute Engine, create instance templates and managed instance groups from source instances, deploy a retail application on managed instance groups, create HTTPS load balancers and health checks for a retail application, and use a Content Delivery Network to cache a retail application's assets. Ready to get started? Let's begin.

**PRIYANKA VERGADIA:** Welcome to Build with Google Cloud, where we build reference architectures. In this video, we will build an e-commerce platform on Google Cloud. What are your options? Stay with me to find out.
[MUSIC PLAYING]

Let's say you're a retailer that sells pet products, shoes, cars, anything. Did you know that digital commerce platform can be built and modernized in multiple ways? I am Priyanka Vergadia, developer advocate here at Google Cloud. And today, let's see what your options are to build a digital e-commerce platform on Google Cloud. You ready? Let's go.
[MOTOR SPINNING]

First, you can build your own e-commerce platform on Google Cloud. Why does this make sense? Because it provides the agility to create new capabilities using data insights and access to machine learning and AI tools within Google Cloud.

There are two ways to build your own platform. You could either migrate your existing e-commerce platform or modernize using containers. Migrate means lifting and shifting your current platform as-is into the Cloud. This is the lowest risk and fastest way to get to Cloud. It is suitable for custom built e-commerce stack leveraging heavy OSS or running on premises. And once you've migrated, you are on your path to modernization.

You could choose to migrate and improve where you can upgrade and improve or optimize using containerised approach. This means you're willing to move to containers and are gradually moving to Google Cloud managed services, such as GKE. It sets the stage for a more seamless modernization path to microservices on GKE.

If the goal is to modernize your commerce application platform completely for benefits of Cloud and speed to market, then you would re-architect to adopt containers using microservices architecture. This helps you shift focus to creating those differentiated customer experiences for digital and omnichannel revenue growth.

We'll come back to architect, the commerce migration and modernization, but before we do that, let's also check out the other options.

The second option is, you can create a headless commerce platform on Google Cloud. This is where you actually decouple the front end, the mobile app, the CMS, et cetera, from your back end or your commerce layer. This approach provides flexibility to your marketing teams to quickly alter front-end elements to run campaigns, promotions, and create most advanced customer experience on your front end. You're building front-end experience on Google Cloud and are integrating with commerce back ends from Google Cloud marketplace offering partners.

This unique approach offers the benefits of the agility to launch or change highly personalized experiences, flexibility to upgrade, update, or replace your dependent systems, and scale different parts of solutions independent of each other. As you can imagine, this approach has a low total cost of ownership, smaller IT teams, and is not constrained by a monolithic roadmap and timelines to offer new storefront features.

Third option? You could use out-of-the-box SaaS solution on Google Cloud marketplace, such as Shopify, BigCommerce, or HCL Commerce. This will work for you if you require minimum customizations and can take advantage of Google Cloud for customer 360 and data insights-driven analytics.

Which of these approaches are you leaning towards to build your e-commerce platform, and why? Share in the comments.

Let's dive into the first two approaches-- build your own and headless commerce-- one by one to understand what it takes to build and which one makes sense for your use case.

Now let's architect the build your own, migrate, and improve approach. This means you're not just migrating but also looking to break down a legacy monolith commerce application into a set of microservices that can help you gain agility in launching those new features in a way to improve customer experience by leveraging data insights that are powered by AI and ML capabilities.

In order to do this, you're going to need to migrate and improve using containerization on Google Kubernetes Engine. Let's see how.

You would deploy your application servers and your database as services and containers inside a GKE cluster. You can accelerate this process of moving to GKE by using Migrate for Anthos, Google's migration automation tool.

All your static content-- including the images, media assets, web files-- will be stored in Cloud Storage. Any other application systems that still exist on premises will be hooked up via a dedicated interconnect Cloud VPN or API management.

Now let's see the request flow.

The user makes a request from their phone, website, or chat application, which then gets resolved by Cloud DNS to a global anycast IP address for Cloud Load Balancing. Cloud Armor works alongside Load Balancing to provide DDoS protection and web application firewall that detects and blocks layer 7 attacks. reCAPTCHA Enterprise provides bot protection. And API defense provides protection for abuse of APIs. Combination of Cloud Armor, Apigee API defense, and reCAPTCHA Enterprise offer the web application and API Protection.

Once the request is validated by these, it is then sent to the web server. The web server then grabs the static content-- such as your images, CSS, your JavaScript-- to render that page from Cloud Storage. It also stores them in Cloud CDN for next request, which helps with performance and latency.

Next, web server would need to interact with the app server for business logic, such as, is this item in-store? The internal load balancing between the web and application servers help distribute the load.

Once the request lands on the app server, it interacts with the database to get the response. Yes, the item is in-store, and there are four more left, or something like that. Gets this information and then sends it back to the user via web servers.

Now, if the request requires the app server to gather information from a service that is on-premises, then it can make a call to the service via an API through Apigee API gateway or VPN over the internet or just direct connectivity to that data center.

If the request was a search or recommendations, then app server interfaces with the Google retail search or the recommendations API to present the appropriate results to the user.

Architecturally, build your own, migrate, and improve provides you the flexibility to add features, ability to scale easily by adding more GKE clusters during traffic spikes. It also offers applications the security using a combination of Cloud Armor, Load Balancer, reCAPTCHA, and API defense.

From the business perspective, the build your own and migrate and improve approach enables the integration with the data platform which creates a strong closed loop between the data platform and the commerce platform. You can collect the first party customer data from shopper interactions on their modern digital commerce platform to use in your data platform for predictive analytics and marketing activation. You can also gain insights from data in data platform to offer personalized shopping experiences and promotions on the digital commerce site.

Are you ready to migrate and improve? We've got a link to the documentation below to help you out.

Now, how would you architect your migration as-is or lift and shift your e-commerce platform to Google Cloud? Say your priority is to move to Google Cloud as quickly as possible or you have a homegrown OSS stack based on-premises, an e-commerce platform that is unique and works well and you just want to migrate. That means you just want to migrate as-is without making any changes. Just get to the Cloud so you can take that first step towards modernization.

For an as-is migration like this, we recommend migrating to Compute Engine virtual machines. Let's see how to architect this.

Your request flow pretty much stays the same, except for your web and application servers and your database. You will create your web and application servers on Compute Engine managed instance groups, which will scale based on the number of requests you receive. You will also deploy your primary and secondary databases on Compute Engine instances.

This architecture offers the ability to scale the web and app tiers independently using the managed instance groups and provides low-latency responses back to the end customer to keep them engaged in their buying journey.

You have the same application security using a combination of Cloud Armor, Load Balancer, reCAPTCHA, and API Defense. In addition, it allows you as a retailer to quickly meet unexpected user traffic spikes without having a large-sized deployment initially, but automatically scale easily up and down enabled by your configuration policies.

Are you ready to migrate your e-commerce application as-is? We've got a link in the documentation to help you out.

Now, let's say you have migrated to Google Cloud using GKE or Compute Engine, or you're building an entire modernized e-commerce platform from scratch. How would you go from here to modernized, fully-containerized microservices architecture? Let's see.

All your requests flow stays the same as you saw earlier. What changes is that now you're going to think about smaller services and separate them out into four layers-- presentation layer, which could be your single-page application builder and content; a services layer, which would include services, such as sessions, search, account, inventory, orders, and anything else that you can think of; the storage layer, which would be your storage choices for those services. There could be different storages based on the kind of service-- for example, Cloud SQL or Spanner for inventory products and maybe Firestore for sessions. Cache layer would be your transient cache using memory store to access recently queried data without querying the storage layer again.

You can now see the flexibility that this architecture allows you to have. You can add and deploy any new services any time without affecting any other services. You can also have different teams working on different services in different programming languages. Lots more flexibility both from the development side and from the business side, which leads to happy developers and happy customers.

And you have enabled your business to have a strong closed loop between the data platform and the commerce platform to understand customer behavior and offer personalized experiences. Another huge advantage of this approach is the seamless integration with other Google Cloud Services, such as Workspace, Maps, G Pay, mobile, Android, Chrome devices, and more.

Are you ready to start your e-commerce modernization journey? We've got a link to a sample code repository for you to get started.

Now, let's take a deep dive into the headless commerce solution.

As a retailer, you might want to own and control the front end for providing an engaging and differentiated shopping experience and leverage the ready-to-use back-end commerce building blocks for capabilities, such as product catalog, pricing, promotions, carts, shipping, accounts, and other things.

In this fast-paced world where user behavior changes every day, headless commerce approach is increasingly preferred for speed-to-market. You can easily launch new experiences quickly and benefit from innovations in back-end capabilities added by our partners on Google Cloud Marketplace, such as Commerce Tools.

How would you architect this? Well, let's see.

The user would have the same secured experience using web application and API security solution. The front end is deployed on GKE or Cloud Run and can use Cloud Storage for static assets, and Firestore or Cloud SQL for your dynamic storage and database options. Interacting with the recommendations API and retail search for an engaging product discovery and shopping experience, that would reduce search abandonment and increase sales conversions. Commerce tools platform from Marketplace serves as the back end for commerce operations. It provides over 300 APIs for e-commerce back-end needs, such as your products, categories, orders, and other such things.

This approach is an alternative to build or buy approach as it offers a right mix of both approaches to realize a solution that enables brand differentiation, flexibility, and agility.

To help you get started with headless commerce, we've built an end-to-end demo. Check it out below and let us know how it goes.

We just looked at the different options to build a modern e-commerce platform on Google Cloud. Depending on your situation, you can choose to migrate as-is to Compute Engine VMs, migrate and improve using containers, build an entirely modern microservices-based architecture creating containerized services, or use partners offered services to adopt headless commerce approach.
Any direction you take is a success and a great start to gain agility in launching new features, innovation, and improving customer experience by leveraging data insights powered by AI and ML, and tools, such as retail search and recommendations AI.
Now, it's your turn. Go build the headless commerce demo with the documentation and a full sample code repository linked below.
If you like this video, then I hope you completely destroy those like and subscribe buttons to let us know. I cannot wait to build more stuff with you.
```
# é›»å­å•†å‹™å¹³å°æ¶æ§‹ç­–ç•¥ - Google Cloud éƒ¨ç½²æŒ‡å—

## ğŸ¯ ä¸‰ç¨®ä¸»è¦éƒ¨ç½²é¸é …

### 1. **è‡ªå»ºå¹³å° (Build Your Own)**
#### A. é·ç§»ä¸¦æ”¹é€² (Migrate & Improve)
- **å®¹å™¨åŒ–é·ç§»**ï¼šä½¿ç”¨ GKE + Migrate for Anthos
- **æ¶æ§‹çµ„æˆ**ï¼š
  - æ‡‰ç”¨ä¼ºæœå™¨ & è³‡æ–™åº« â†’ GKE å®¹å™¨
  - éœæ…‹å…§å®¹ â†’ Cloud Storage
  - æ··åˆé›²é€£æ¥ â†’ Cloud VPN / API ç®¡ç†

#### B. ç›´æ¥é·ç§» (Lift & Shift)
- **é©ç”¨æƒ…å¢ƒ**ï¼šå¿«é€Ÿä¸Šé›²ã€ç¾æœ‰ OSS å †ç–Š
- **æŠ€è¡“å¯¦ç¾**ï¼š
  - Web/App ä¼ºæœå™¨ â†’ Compute Engine è¨—ç®¡åŸ·è¡Œå€‹é«”ç¾¤çµ„
  - è³‡æ–™åº« â†’ Compute Engine åŸ·è¡Œå€‹é«”

### 2. **ç„¡é ­å•†å‹™ (Headless Commerce)**
- **æ ¸å¿ƒæ¦‚å¿µ**ï¼šå‰å¾Œç«¯åˆ†é›¢
- **å‰ç«¯**ï¼šGKE/Cloud Run + Cloud Storage + Firestore/Cloud SQL
- **å¾Œç«¯**ï¼šGoogle Cloud Marketplace åˆä½œå¤¥ä¼´æœå‹™ (å¦‚ Commerce Tools)

### 3. **SaaS è§£æ±ºæ–¹æ¡ˆ**
- **é¸é …**ï¼šShopifyã€BigCommerceã€HCL Commerce
- **å„ªå‹¢**ï¼šæœ€å°å®¢è£½åŒ–éœ€æ±‚ï¼Œæ•´åˆ Google Cloud æ•¸æ“šåˆ†æ

## ğŸ”„ è«‹æ±‚æµç¨‹æ¶æ§‹

### å®‰å…¨å±¤å±¤æŠŠé—œï¼š
```
ç”¨æˆ¶è«‹æ±‚ â†’ Cloud DNS â†’ Cloud Load Balancing
         â†’ Cloud Armor (DDoS/WAF é˜²è­·)
         â†’ reCAPTCHA Enterprise (æ©Ÿå™¨äººé˜²è­·)
         â†’ Apigee API Defense (API æ¿«ç”¨é˜²è­·)
```

### æ‡‰ç”¨è™•ç†æµç¨‹ï¼š
1. **Web ä¼ºæœå™¨**ï¼šå¾ Cloud Storage/CDN è¼‰å…¥éœæ…‹è³‡æº
2. **æ‡‰ç”¨ä¼ºæœå™¨**ï¼šè™•ç†æ¥­å‹™é‚è¼¯ï¼ŒæŸ¥è©¢è³‡æ–™åº«
3. **è³‡æ–™å±¤**ï¼šè³‡æ–™åº«æŸ¥è©¢ + å¤–éƒ¨æœå‹™æ•´åˆ
4. **é€²éšåŠŸèƒ½**ï¼šé›¶å”®æœå°‹ã€æ¨è–¦ APIã€æ··åˆé›²æœå‹™

## ğŸ—ï¸ ç¾ä»£åŒ–å¾®æœå‹™æ¶æ§‹

### å››å±¤æ¶æ§‹è¨­è¨ˆï¼š
| å±¤ç´š | çµ„ä»¶ | æŠ€è¡“é¸æ“‡ |
|-----|------|----------|
| **è¡¨ç¾å±¤** | SPAã€å…§å®¹ç®¡ç† | å‰ç«¯æ¡†æ¶ |
| **æœå‹™å±¤** | Sessionã€Searchã€Accountã€Inventoryã€Orders | å¾®æœå‹™ |
| **å„²å­˜å±¤** | æŒä¹…åŒ–è³‡æ–™ | Cloud SQLã€Spannerã€Firestore |
| **å¿«å–å±¤** | æš«å­˜è³‡æ–™ | Memorystore |

### ç¾ä»£åŒ–å„ªå‹¢ï¼š
- âœ… ç¨ç«‹éƒ¨ç½²æ–°æœå‹™
- âœ… å¤šåœ˜éšŠä¸¦è¡Œé–‹ç™¼
- âœ… æŠ€è¡“æ£§éˆæ´»æ€§
- âœ… ç„¡ç¸«æ•´åˆ Google ç”Ÿæ…‹

## ğŸ’¡ æ¥­å‹™åƒ¹å€¼èˆ‡å„ªå‹¢

### æŠ€è¡“å„ªå‹¢ï¼š
- **å½ˆæ€§æ“´å±•**ï¼šè‡ªå‹•æ‡‰å°æµé‡é«˜å³°
- **å®‰å…¨é˜²è­·**ï¼šå¤šå±¤æ¬¡å®‰å…¨é˜²è­·
- **æ•ˆèƒ½å„ªåŒ–**ï¼šä½å»¶é²å›æ‡‰ï¼Œæå‡ç”¨æˆ¶é«”é©—

### å•†æ¥­åƒ¹å€¼ï¼š
- **æ•¸æ“šé–‰ç’°**ï¼šå®¢æˆ¶æ•¸æ“š â†’ åˆ†ææ´å¯Ÿ â†’ å€‹æ€§åŒ–é«”é©—
- **å‰µæ–°åŠ é€Ÿ**ï¼šå¿«é€Ÿæ¨å‡ºæ–°åŠŸèƒ½
- **æˆæœ¬å„ªåŒ–**ï¼šæŒ‰éœ€æ“´å±•ï¼Œé™ä½ TCO

## ğŸš€ æ¨è–¦ä½¿ç”¨æƒ…å¢ƒ

| æƒ…å¢ƒ | æ¨è–¦æ–¹æ¡ˆ | é—œéµå„ªå‹¢ |
|------|----------|----------|
| **å¿«é€Ÿé·ç§»** | Compute Engine ç›´æ¥é·ç§» | é¢¨éšªä½ã€é€Ÿåº¦å¿« |
| **é€æ­¥ç¾ä»£åŒ–** | GKE å®¹å™¨åŒ–é·ç§» | å¹³è¡¡é¢¨éšªèˆ‡å‰µæ–° |
| **å…¨æ–°å»ºç½®** | å¾®æœå‹™æ¶æ§‹ | æœ€å¤§éˆæ´»æ€§ |
| **è¡ŒéŠ·é©…å‹•** | ç„¡é ­å•†å‹™ | å‰ç«¯å‰µæ–°é€Ÿåº¦å¿« |

## ğŸ“š è³‡æºèˆ‡ä¸‹ä¸€æ­¥
- æ–‡ä»¶é€£çµï¼šé·ç§»æŒ‡å—ã€æ¨£æœ¬ä»£ç¢¼åº«
- å¯¦ä½œæ¼”ç¤ºï¼šç„¡é ­å•†å‹™ç«¯åˆ°ç«¯æ¼”ç¤º
- åˆä½œå¤¥ä¼´ï¼šCommerce Tools ç­‰ Marketplace è§£æ±ºæ–¹æ¡ˆ

---

**ç¸½çµ**ï¼šç„¡è«–é¸æ“‡å“ªç¨®è·¯å¾‘ï¼ŒGoogle Cloud éƒ½æä¾›å®Œæ•´çš„å·¥å…·éˆå’Œæœå‹™ç”Ÿæ…‹ï¼Œæ”¯æ´é›»å­å•†å‹™å¹³å°çš„ç¾ä»£åŒ–è½‰å‹å’ŒæŒçºŒå‰µæ–°ã€‚



# lession2
```

As mentioned in the introduction to the course, there is a spectrum of different options in Google Cloud for compute and processing. We will focus on the traditional virtual machine instances. Now the difference is, Compute Engine gives you the utmost in flexibility: run whatever language you wantâ€”it's your virtual machine. This is purely an infrastructure as a service or IaaS model. You have a VM and an operating system, and you can choose how to manage it and how to handle aspects, such as autoscaling, where youâ€™ll configure the rules about adding more virtual machines in specific situations. Autoscaling will be covered later in the course. The primary work case of Compute Engine is any general workload, especially an enterprise application that was designed to run on a server infrastructure. This makes Compute Engine very portable and easy to run in the cloud. Other services, like Google Kubernetes Engine, which consists of containerized workloads, may not be as easily transferable as what youâ€™re used to from on-premises.

So what is Compute Engine? At its heart, it's physical servers that you're used to, running inside the Google Cloud environment, with a number of different configurations. Both predefined and custom machine types allow you to choose how much memory and how much CPU you want. You choose the type of disk you want, whether you want to use persistent disks backed up by standard hard drives or solid-state drives, local SSDs, Cloud Storage, or a mix. You can even configure the networking interfaces and run a combination of Linux and Windows machines. We will discuss these options in more detail later in the module. Several different features will be covered throughout this module, such as machine rightsizing, startup scripts, metadata, availability policies, OS patch management, and pricing and usage discounts.

Letâ€™s start by looking at the compute options. Compute Engine provides several different machine types that weâ€™ll discuss later in this module. If those machines donâ€™t meet your needs, you can also customize your own machine. Your choice of CPU will affect your network throughput. Specifically, your network will scale at 2 gigabits per second for each CPU core, except for instances with 2 or 4 CPUs which receive up to 10 gigabits per second of bandwidth. As of this recording, there is a theoretical maximum throughput of 100 gigabits per second for an instance with 224 vCPU, when you choose an N2D machine series. When you're migrating from an on-premises setup, you're used to physical cores, which have hyperthreading. On Compute Engine, each virtual CPU (or vCPU) is implemented as a single hardware hyper-thread on one of the available CPU Platforms. For an up-to-date list of all the available CPU platforms, refer to the links section of this video [[https://cloud.google.com/compute/docs/cpu-platforms](https://cloud.google.com/compute/docs/cpu-platforms)]

After you pick your compute options, you want to choose your disk. You have three options: Standard, SSD, or local SSD. So basically, do you want the standard spinning hard disk drives (HDDs), or flash memory solid-state drives (SSDs)? Both of these options provide the same amount of capacity in terms of disk size when choosing a persistent disk. Therefore, the question really is about performance versus cost, because there's a different pricing structure. Basically, SSDs are designed to give you a higher number of IOPS per dollar versus standard disks, which will give you a higher amount of capacity for your dollar. Local SSDs have even higher throughput and lower latency than SSD persistent disks, because they are attached to the physical hardware. However, the data that you store on local SSDs persists only until you stop or delete the instance. Typically, a local SSD is used as a swap disk, just like you would do if you want to create a ramdisk, but if you need more capacity, you can store those on a local SSD. You can create instances with up to eight separate 375-GB local SSD partitions for a total of 3 TB of local SSD space for each instance. Standard and non-local SSD disks can be sized up to 257 TB for each instance. The performance of these disks scales with each GB of space allocated.

As for networking, we have already seen networking features applied to Compute Engine in the previous moduleâ€™s lab. We looked at the different types of networks and created firewall rules using IP addresses and network tags. Youâ€™ll also notice that you can do regional HTTPS load balancing and network load balancing. This doesnâ€™t require any pre-warming because a load balancer isn't a hardware device that needs to analyze your traffic. A load balancer is essentially a set of traffic engineering rules that are coming into the Google network, and VPC is applying your rules destined for your IP address subnet range. Weâ€™ll learn more about load balancers in a later course of the Architecting with Google Compute Engine series.

Now that you have completed the lab, letâ€™s dive deeper into the compute options that are available to you in GCP by focusing on CPU and memory. You have three options for creating and configuring a VM. You can use the GCP Console as you did in the previous lab, the Cloud Shell command line, or the RESTful API. If youâ€™d like to automate and process very complex configurations, you might want to programmatically configure these through the RESTful API by defining all the different options for your environment. If you plan on using the command line or RESTful API, I recommend that you first configure the instance through the GCP console and then ask Compute Engine for the equivalent REST request or command line, as shown in the demo earlier. This way you avoid any typos and get dropdown lists of all the available CPU and memory options.

Speaking of CPU and memory options, letâ€™s look at the different machine types that are currently available. When you create a VM, you select a machine type from a machine family that determines the resources available to that VM. There are several machine families you can choose from and each machine family is further organized into machine series and predefined machine types within each series. A machine family is a curated set of processor and hardware configurations optimized for specific workloads. When you create a VM instance, you choose a predefined or custom machine type from your preferred machine family. Alternatively, you can create custom machine types. These let you specify the number of vCPUs and the amount of memory for your instance. There are four Compute Engine machine families: General-purpose, Compute-optimized, Memory-optimized, and Accelerator-optimized.

The general-purpose machine family has the best price-performance with the most flexible vCPU to memory ratios, and provides features that target most standard and cloud-native workloads. The E2 machine series is suited for day-to-day computing at a lower cost, especially where there are also no application dependencies on a specific CPU architecture. E2 VMs provide a variety of compute resources for the lowest price on Compute Engine, especially when paired with committed-use discounts. You simply pick the amount of vCPU and memory that you want, and Google provisions it for you. Standard E2 VMs have between 2 to 32 vCPUs with a ratio of 0.5 GB to 8 GB of memory per vCPU. They are a great choice for web servers, small to medium databases, development and test environments, and many applications that don't have strict performance requirements. They offer a compatible performance baseline with the current N1 VMs for those of you who have been using them. The E2 machine series also contains shared-core machine types that use context-switching to share a physical core between vCPUs for multitasking. Different shared-core machine types sustain different amounts of time on a physical core. In general, shared-core machine types can be more cost-effective for running small, non-resource intensive applications than standard, high-memory, or high-CPU machine types. Shared-core E2 machine types have 0.25 to 1 vCPUs with 0.5 GB to 8 GB of memory.

N2 and N2D are the next generation following N1 VMs, offering a significant performance jump. N2 and N2D are the most flexible VM types and provide a balance between price and performance across a wide range of VM shapes, including enterprise applications, medium-to-large databases, and many web and app-serving workloads. Committed use and sustained use discounts are supported. N2 VMs support the latest second generation scalable processor from Intel with up to 128 vCPUs and 0.5 to 8 GB of memory per vCPU. At the time of recording, Cascade Lake is the default processor for machine types up to 80 vCPUs. In beta, for larger machine types Ice Lake is the default processor for specific regions and zones. N2D are AMD-based general purpose VMs. They leverage the latest EPYC Milan and EPYC Rome processors, and provide up to 224 vCPUs per node. Tau T2D VMs are optimized for cost-effective performance of demanding scale-out workloads. T2D VMs are built on the latest 3rd Gen AMD EPYC processors and offer full x86 compatibility. They are suited for scale-out workloads including web servers, containerized microservices, media transcoding, and large-scale Java applications. T2D VMs come in predefined VM shapes, with up to 60 vCPUs per VM and 4 GB of memory per vCPU. If you have containerized workloads, Tau VMs are supported by Google Kubernetes Engine to help optimize price-performance. You can add T2D nodes to your GKE clusters by specifying the T2D machine type in your GKE node pools.

The compute-optimized machine family has the highest performance per core on Compute Engine and is optimized for compute-intensive workloads. C2 VMs are the best fit VM type for compute-intensive workloads, including AAA gaming, electronic design automation, and high-performance computing across simulations, genomic analysis, or media transcoding. They might also be applications that have very expensive per core licensing and thus would benefit from higher per core performance. Powered by high-frequency Intel-scalable processors, Cascade Lake, C2 machine types offer up to 3.8 Ghz sustained all-core turbo and provide full transparency into the architecture of the underlying server platforms, enabling advanced performance tuning. The C2 series comes in different machine types ranging from 4 to 60 vCPUs, and offers up to 240 GB of memory. You can also attach up to 3 TB of local storage to these VMs for applications that require higher storage performance. The C2D machine type series provides the largest VM sizes and are best-suited for high-performance computing (HPC). The C2D series also has the largest available last-level cache (LLC) cache per core. The C2D machine series come in different machine types ranging from 2 to 112 vCPUs, and offers 4 GB of memory per vCPU core. You can also attach up to 3TB of local storage to these machine types for applications that require higher storage performance. C2D VMs are available on the third generation AMD EPYC Milan platform.

The memory-optimized machine family provides the most compute and memory resources of any Compute Engine machine family offering. They are ideal for workloads that require higher memory-to-vCPU ratios than the high-memory machine types in the general-purpose machine family. The M1 machine series has up to 4 TB of memory, while the M2 machine series has up to 12 TB of memory. These machine series are well-suited for large in-memory databases such as SAP HANA, as well as in-memory data analytics workloads. Both the M1 and M2 machine series offer the lowest cost per GB of memory on Compute Engine, making them a great choice for workloads that utilize higher memory configurations with low compute resource requirements. Additionally, they offer up to 30% sustained use discounts and are also eligible for committed use discounts, bringing additional savings of greater than 60% for three-year commitments.

The accelerator-optimized machine family is ideal for massively parallelized Compute Unified Device Architecture (CUDA) compute workloads, such as machine learning (ML) and high-performance computing (HPC). This family is the optimal choice for workloads that require GPUs. The A2 series has 12 to 96 vCPUs, and up to 1360 GB of memory. Each A2 machine type has a fixed number (up to 16) of NVIDIAâ€™s Ampere A100 GPUs. An A100 GPU provides 40 GB of GPU memoryâ€”ideal for large language models, databases, and HPC. Additional information, including the latest specs for currently available VM machine types, can be found in the machine types documentation.

If none of the predefined machine types match your needs, you can independently specify the number of vCPUs and the amount of memory for your instance. Custom machine types are ideal for the following scenarios: When you have workloads that are not a good fit for the predefined machine types that are available to you. Or when you have workloads that require more processing power or more memory, but don't need all of the upgrades that are provided by the next larger predefined machine. It costs slightly more to use a custom machine type than an equivalent predefined machine type, and there are still some limitations of memory and vCPUs you can select: only machine types with 1 vCPU or an even number of vCPUs can be created. Memory must be between 0.9 GB and 6.5 GB per vCPU (by default). The total memory of the instance must be a multiple of 256 MB. By default, a custom machine can have up to 6.5 GB of memory per vCPU. However, this might not be enough memory for your workload. At an additional cost, you can get more memory per vCPU beyond the 6.5 GB limit. This is referred to as extended memory, and you can learn more about this in the link provided in the module PDF located in the Course Resources.

The first thing you want to consider when choosing a region and zone is the geographical location in which you want to run your resources. This map shows the current and planned Google Cloud regions and number of zones. For up-to-date information on the available regions and zones, see the documentation linked for this video. Each zone supports a combination of Ivy Bridge, Sandy Bridge, Haswell, Broadwell, and Skylake platforms. When you create an instance in the zone, your instance will use the default processor supported in that zone. For example, if you create an instance in the us-central1-a zone, your instance will use a Sandy Bridge processor.

Now that we have covered all the different compute image and disk options, let's look at some common actions that you can perform with Compute Engine. Every VM instance stores its metadata on a metadata server. The metadata server is particularly useful in combination with startup and shutdown scripts because you can use the metadata server to programmatically get unique information about an instance without additional authorization. For example, you can write a startup script that gets the metadata key value pair for an instance's external IP address and use that IP address in your script to setup a database. Because the default metadata keys are the same on every instance, you can reuse your script without having to update it for each instance, this helps you create less brittle code for your applications. Storing and retrieving instance metadata is a very common Compute Engine action. I recommend storing these startup and shutdown scripts in Cloud Storage as you will explore in the upcoming lab of this module.

Another common action is to move an instance to a new zone. For example, you might do so for geographical reasons or because a zone is being deprecated. You can move a VM even if one of the following scenarios apply: The VM instance is in a TERMINATED state, or The VM instance is a Shielded VM that uses UEFI firmware. If you move your instance within the same region, you can automate the move by using the gcloud compute instances move command. To move your VM, you must shut down the VM, move it to the destination zone or region, and then restart it. After you move your VM, update any references that you have to the original resource, such as any target VMs or target pools that point to the earlier VM. During the move, some server-generated properties of your VM and disks change. If we move your instance to a different region, you need to manually do so by following the process outlined here. This involves making a snapshot of all persistent disks and creating new disks in the destination zone from that snapshot. Next, you create a new VM in the destination zone and attach the new persistent disks, assign a static IP, and update any references to the VM. Finally, you delete the original VM, its disks and the snapshot.

Speaking of snapshots, let's take a closer look at these. Snapshots have many use cases. For example, they can be used to backup critical data into a durable storage solution to meet application, availability, and recovery requirements. These snapshots are stored in Cloud Storage, which is covered later. Snapshots can also be used to move data between zones. I just discussed this when going over the manual process of moving an instance between two regions, but this can also be used to simply transfer data from one zone to another. For example, you might want to minimize latency by migrating data to a drive that can be locally attached in the zone where it is used. Which brings me to another snapshot use case of transferring data to a different disk type. For example, if you want to improve disk performance, you could use a snapshot to transfer data from a standard ECD persistent disk to a SSD persistent disk.

Now that I've covered some of these snapshot use cases, let's explore the concept of a disk snapshot. First of all, this slide is titled persistent disk snapshots because snapshots are available only to persistent disks and not to local SSDs. Snapshots are different from public images and custom images which are used primarily to create instances or configure instance templates, in that snapshots are useful for periodic backup of the data on your persistent disks. Snapshots are incremental and automatically compressed, so you can create regular snapshots on a persistent disk faster and at a much lower cost than if you regularly created a full image of the disk. As we saw with the previous examples, snapshots can be restored to a new persistent disk, allowing for a move to a new zone. To create a persistent disk snapshot, see the link section of this video. Another common Compute Engine action is to resize your persistent disk. The added benefit of increasing storage capacity is to improve I/O performance. This can be achieved while the disk is attached to a running VM without having to create a snapshot. Now, while you can grow disk and size, you can never shrink them. So keep this in mind.

A managed instance group is a collection of identical VM instances that you control as a single entity using an instance template. You can easily update all the instances in a group by specifying a new template in a rolling update. Also when your applications require additional compute resources, managed instance groups can scale automatically to the number of instances in the group. Managed instance groups can work with load balancing services to distribute network traffic to all of the instances in the group. If an instance in the group stops, crashes or is deleted by an action other than the instance group commands, the managed instance group automatically recreates the instance so it can resume its processing tasks. The recreated instance uses the same name and the same instance template as the previous instance. Managed instance groups can automatically identify and recreate unhealthy instances in a group to ensure that all instances are running optimally. Regional managed instance groups are generally recommended over zonal managed instance groups because they allow you to spread the application load across multiple zones instead of confining your application to a single zone or having you manage multiple instance groups across different zones. This replication protects against zonal failures and unforeseen scenarios where an

```
# Google Compute Engine (GCE) é‡é»ç­†è¨˜

## 1. Compute Engine åŸºæœ¬æ¦‚å¿µ
- **IaaS æ¨¡å‹**ï¼šæä¾›æœ€å¤§éˆæ´»æ€§ï¼Œå¯é‹è¡Œä»»ä½•èªè¨€
- **ä¸»è¦ç”¨ä¾‹**ï¼šä¼æ¥­æ‡‰ç”¨ç¨‹å¼ã€è¨­è¨ˆåœ¨ä¼ºæœå™¨åŸºç¤æ¶æ§‹ä¸Šé‹è¡Œçš„æ‡‰ç”¨
- **å¯ç§»æ¤æ€§**ï¼šå®¹æ˜“å¾æœ¬åœ°é·ç§»åˆ°é›²ç«¯

## 2. è¨ˆç®—é¸é … (Compute Options)

### æ©Ÿå™¨é¡å‹ (Machine Types)
**å››å¤§æ©Ÿå™¨ç³»åˆ—ï¼š**

#### A. é€šç”¨å‹ (General-purpose)
- **E2 ç³»åˆ—**ï¼šæ—¥å¸¸è¨ˆç®—ï¼Œæˆæœ¬æœ€ä½
  - 2-32 vCPUsï¼Œ0.5-8 GB è¨˜æ†¶é«”/vCPU
  - é©ç”¨ï¼šç¶²é ä¼ºæœå™¨ã€ä¸­å°å‹è³‡æ–™åº«ã€é–‹ç™¼æ¸¬è©¦ç’°å¢ƒ
  - å…±äº«æ ¸å¿ƒé¡å‹ï¼š0.25-1 vCPUs

- **N2/N2D ç³»åˆ—**ï¼šä¸‹ä¸€ä»£ VMï¼Œæ•ˆèƒ½æå‡
  - N2ï¼šIntel è™•ç†å™¨ï¼Œæœ€å¤š 128 vCPUs
  - N2Dï¼šAMD è™•ç†å™¨ï¼Œæœ€å¤š 224 vCPUs

- **T2D ç³»åˆ—**ï¼šAMD EPYC è™•ç†å™¨
  - æœ€å¤š 60 vCPUsï¼Œ4 GB è¨˜æ†¶é«”/vCPU
  - é©ç”¨ï¼šå®¹å™¨åŒ–å·¥ä½œè² è¼‰ã€åª’é«”è½‰ç¢¼

#### B. è¨ˆç®—å„ªåŒ–å‹ (Compute-optimized)
- **C2 ç³»åˆ—**ï¼šIntel Cascade Lake
  - 4-60 vCPUsï¼Œæœ€å¤š 240 GB è¨˜æ†¶é«”
  - é©ç”¨ï¼šéŠæˆ²ã€é«˜æ•ˆèƒ½è¨ˆç®—ã€åª’é«”è½‰ç¢¼

- **C2D ç³»åˆ—**ï¼šAMD EPYC Milan
  - 2-112 vCPUsï¼Œ4 GB è¨˜æ†¶é«”/vCPU

#### C. è¨˜æ†¶é«”å„ªåŒ–å‹ (Memory-optimized)
- **M1 ç³»åˆ—**ï¼šæœ€å¤š 4 TB è¨˜æ†¶é«”
- **M2 ç³»åˆ—**ï¼šæœ€å¤š 12 TB è¨˜æ†¶é«”
- é©ç”¨ï¼šSAP HANAã€è¨˜æ†¶é«”å…§è³‡æ–™åˆ†æ

#### D. åŠ é€Ÿå™¨å„ªåŒ–å‹ (Accelerator-optimized)
- **A2 ç³»åˆ—**ï¼š12-96 vCPUs
- æœ€å¤š 16 å€‹ NVIDIA A100 GPUs
- é©ç”¨ï¼šæ©Ÿå™¨å­¸ç¿’ã€é«˜æ•ˆèƒ½è¨ˆç®—

### è‡ªè¨‚æ©Ÿå™¨é¡å‹
- å¯ç¨ç«‹æŒ‡å®š vCPU æ•¸é‡å’Œè¨˜æ†¶é«”å¤§å°
- é™åˆ¶ï¼švCPU å¿…é ˆç‚º 1 æˆ–å¶æ•¸ï¼Œè¨˜æ†¶é«”å¿…é ˆæ˜¯ 256 MB çš„å€æ•¸

## 3. å„²å­˜é¸é … (Disk Options)

### ä¸‰ç¨®ç£ç¢Ÿé¡å‹ï¼š
1. **æ¨™æº–ç¡¬ç¢Ÿ (Standard HDD)**ï¼šå®¹é‡å¤§ï¼Œæˆæœ¬ä½
2. **SSD æŒä¹…ç£ç¢Ÿ**ï¼šIOPS æ•ˆèƒ½é«˜
3. **æœ¬åœ° SSD**ï¼šååé‡æœ€é«˜ï¼Œå»¶é²æœ€ä½
   - è³‡æ–™åœ¨åœæ­¢/åˆªé™¤å¯¦ä¾‹æ™‚æœƒæ¶ˆå¤±
   - æœ€å¤š 3 TB (8Ã—375 GB)

### ç£ç¢Ÿè¦æ ¼ï¼š
- æ¨™æº–å’Œ SSD ç£ç¢Ÿï¼šæœ€å¤š 257 TB
- æ•ˆèƒ½éš¨åˆ†é…ç©ºé–“å¤§å°æ“´å±•

## 4. ç¶²è·¯åŠŸèƒ½
- ç¶²è·¯ååé‡ï¼šæ¯æ ¸å¿ƒ 2 Gbps
- 2-4 CPU å¯¦ä¾‹ï¼šæœ€å¤š 10 Gbps
- æœ€å¤§ç†è«–ååé‡ï¼š100 Gbps (224 vCPU N2D ç³»åˆ—)

## 5. é‡è¦åŠŸèƒ½

### å¿«ç…§ (Snapshots)
- åƒ…é©ç”¨æ–¼æŒä¹…ç£ç¢Ÿ
- å¢é‡å‚™ä»½ï¼Œè‡ªå‹•å£“ç¸®
- ç”¨é€”ï¼šè³‡æ–™å‚™ä»½ã€è·¨å€åŸŸé·ç§»ã€è®Šæ›´ç£ç¢Ÿé¡å‹

### åŸ·è¡Œå€‹é«”ç¾¤çµ„ (Instance Groups)
- ç®¡ç†ç›¸åŒ VM çš„é›†åˆ
- è‡ªå‹•æ“´å±•ã€å¥åº·æª¢æŸ¥ã€è‡ªå‹•æ¢å¾©
- æ¨è–¦ä½¿ç”¨å€åŸŸæ€§ç®¡ç†åŸ·è¡Œå€‹é«”ç¾¤çµ„

### ä¸­ç¹¼è³‡æ–™ä¼ºæœå™¨
- å„²å­˜å¯¦ä¾‹å…ƒè³‡æ–™
- èˆ‡å•Ÿå‹•/é—œé–‰è…³æœ¬é…åˆä½¿ç”¨
- å¯å–å¾—å¯¦ä¾‹å”¯ä¸€è³‡è¨Šï¼ˆå¦‚å¤–éƒ¨ IPï¼‰

## 6. é·ç§»èˆ‡ç®¡ç†

### ç§»å‹•å¯¦ä¾‹åˆ°æ–°å€åŸŸï¼š
- è‡ªå‹•ï¼ˆåŒå€åŸŸï¼‰ï¼šä½¿ç”¨ `gcloud compute instances move`
- æ‰‹å‹•ï¼ˆè·¨å€åŸŸï¼‰ï¼šéœ€å»ºç«‹å¿«ç…§ã€åœ¨æ–°å€åŸŸå»ºç«‹ç£ç¢Ÿå’Œå¯¦ä¾‹

### ç£ç¢Ÿèª¿æ•´ï¼š
- å¯å¢åŠ å®¹é‡ï¼ˆä¸èƒ½æ¸›å°‘ï¼‰
- åœ¨é‹è¡Œæ™‚å¯èª¿æ•´ï¼Œç„¡éœ€å¿«ç…§

## 7. æˆæœ¬å„ªåŒ–
- æŒçºŒä½¿ç”¨æŠ˜æ‰£ï¼šæœ€é«˜ 30%
- æ‰¿è«¾ä½¿ç”¨æŠ˜æ‰£ï¼šä¸‰å¹´ç¯€çœè¶…é 60%
- E2 ç³»åˆ—é…åˆæ‰¿è«¾ä½¿ç”¨æŠ˜æ‰£æˆæœ¬æœ€ä½

## 8. å€åŸŸèˆ‡å¯ç”¨å€é¸æ“‡
- è€ƒæ…®åœ°ç†ä½ç½®éœ€æ±‚
- ä¸åŒå€åŸŸæ”¯æ´ä¸åŒ CPU å¹³å°
- æª¢è¦–æœ€æ–°å¯ç”¨å€åŸŸæ–‡ä»¶ä»¥ç²å¾—æœ€æ–°è³‡è¨Š

é€™ä»½ç­†è¨˜æ¶µè“‹äº† Compute Engine çš„æ ¸å¿ƒæ¦‚å¿µã€é…ç½®é¸é …å’Œç®¡ç†åŠŸèƒ½ï¼Œé©åˆå¿«é€Ÿè¤‡ç¿’å’Œåƒè€ƒä½¿ç”¨ã€‚