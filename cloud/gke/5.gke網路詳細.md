# Google Kubernetes Engine (GKE) 网络架构详解

## 一、核心架构概述
1. **基础网络模型**
   - 基于Google全球骨干网（低延迟 <5ms）
   - 每个GKE集群部署在独立的VPC网络中
   - 采用CNI标准（默认使用kubenet插件）

2. **多平面分离设计**
   - 控制平面（Control Plane）：
     - 托管式服务（自动分配IP）
     - 通过Private Service Connect实现私有访问
   - 数据平面（Data Plane）：
     - 节点间通信使用VPC原生路由
     - Pod间通信通过覆盖网络(Overlay)

## 二、IP地址分配机制

### 节点级分配
| 分配对象 | CIDR范围示例 | 说明 |
|---------|-------------|------|
| 主网卡 | 10.0.0.0/24 | 从VPC子网自动分配 |
| Pod网段 | 10.4.1.0/24 | 每个节点独立CIDR块 |
| 服务网段 | 10.96.0.0/20 | 集群创建时指定 |

> 实际案例：  
> 当节点10.0.0.2宕机时，其Pod IP段10.4.1.0/24会被标记为不可用，调度器会避免将新Pod分配到该网段

### Pod IP分配策略
- **标准模式**：
  - 每个节点固定分配/24网段（256个IP）
  - 实际可调度Pod数 = IP数 - 预留缓冲（默认-16）
  
- **灵活模式**：
  ```yaml
  gcloud container clusters create \
    --enable-ip-alias \
    --cluster-secondary-range-name="pod-range" \
    --services-secondary-range-name="svc-range" 
  ```
## 三、服务网络实现
```
Service类型对比
类型	ClusterIP	NodePort	LoadBalancer	Headless
特点	内部VIP	端口暴露	云LB集成	DNS直连
适用场景	集群内通信	开发测试	生产环境	StatefulSet
访问范围	仅集群内	节点IP+端口	公网/内网LB	直接Pod IP
典型延迟	<1ms	1-2ms	3-5ms	<1ms
后端选择算法	随机(RR)	会话保持	基于地域	DNS轮询

```
### kube-proxy工作流
1. 监听API Server获取Endpoint变化
2. 生成iptables规则链：
    - KUBE-SERVICES（入口链）
    - KUBE-SVC-XXXX（服务链）
    - KUBE-SEP-XXXX（端点链）

3. 采用随机均衡算法(RR)转发
    - 典型问题排查：
    ```
    当Service无法访问时，需检查：
    1. iptables -t nat -L KUBE-SERVICES
    2. Endpoint是否健康 kubectl get ep
    ```
## 四、高级网络功能
### 网络策略(NetworkPolicy)
```
yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-access
spec:
  podSelector:
    matchLabels:
      role: db
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: app
    ports:
    - protocol: TCP
      port: 5432
```     
### 跨集群通信方案
1. VPC对等连接
    - 要求非重叠CIDR
    - 最大传输单元(MTU)需≤1460字节
2. Anthos Service Mesh
    - 基于Envoy实现跨集群LB
    - 自动mTLS加密
## 五、安全实践
1. 防火墙规则配置
```
方向	必需规则	协议
出站	*.googleapis.com:443	TCP
出站	*.gcr.io:443	TCP
入站	130.211.0.0/22	TCP/6443
```
### 私有集群最佳实践
1. 启用专用端点
```
gcloud container clusters update CLUSTER_NAME \
  --enable-private-endpoint
```
2. 配置Cloud NAT避免节点出站公网IP
3. 使用Identity Aware Proxy访问控制平面

## 六、性能优化
### 网络参数调优
```
# 调整conntrack表大小
sysctl -w net.netfilter.nf_conntrack_max=1000000

# 启用Pod直接路由
annotations:
  networking.gke.io/node-network-deny-all: "false"
```
### 监控指标
- 关键Prometheus指标：
```
kube_pod_network_in_bytes
kube_service_spec_type
networkpolicy_rule_count
```
## Prometheus监控指标详解
```
指标名称	类型	说明	告警阈值建议
kube_pod_network_in_bytes	Gauge	Pod累计接收字节数	同比增长>300%时告警
kube_service_spec_type	Enum	服务类型(1=ClusterIP,2=NodePort等)	用于服务类型分布统计
networkpolicy_rule_count	Gauge	当前生效的网络策略规则数	单节点>500时告警
```
1. 网络吞吐量分析
```
# 接收流量TOP10 Pods
topk(10, sum by (pod, namespace) (
  rate(container_network_receive_bytes_total{cluster="$cluster"}[5m])
)

# 发送流量异常检测
sum by (pod) (
  rate(container_network_transmit_bytes_total{cluster="$cluster"}[5m])
) > 1000000  # 1MB/s阈值
```
2. 连接跟踪监控
```
# 连接跟踪利用率
(
  node_nf_conntrack_entries{instance=~".*"}
  /
  node_nf_conntrack_entries_limit{instance=~".*"}
) * 100 > 80  # 超过80%利用率告警

# 连接数异常Pod
topk(3, sum by (pod_name) (
  increase(netfilter_conntrack_entries{status="estimated"}[1h])
))
```
3. Service延迟分析
```
# 按服务分类的P99延迟
histogram_quantile(0.99,
  sum by (le, service) (
    rate(apiserver_request_duration_seconds_bucket{resource="services"}[5m])
  )
)

# 延迟突增检测
(
  histogram_quantile(0.99,
    rate(apiserver_request_duration_seconds_bucket{resource="services"}[5m])
  )
  / 
  (
    histogram_quantile(0.99,
      rate(apiserver_request_duration_seconds_bucket{resource="services"}[5m] offset 1h)
    )
  ) > 1.5  # 相比1小时前增长50%
```

## 实战监控面板配置
### Grafana面板JSON片段
```
{
  "panels": [
    {
      "title": "网络流量热力图",
      "type": "heatmap",
      "targets": [{
        "expr": "sum by (pod) (rate(container_network_receive_bytes_total[1m]))",
        "format": "heatmap"
      }]
    },
    {
      "title": "Service延迟分布",
      "type": "histogram",
      "targets": [{
        "expr": "histogram_quantile(0.95, sum(rate(apiserver_request_duration_seconds_bucket[5m])) by (le, service))"
      }]
    }
  ]
}
```
### 常用诊断命令速查
```
网络连通性测试工具集
场景	命令	备注
DNS解析	> kubectl exec -it test-pod -- dig +short <service>.<ns>.svc.cluster.local	需安装dnsutils
端口连通性	> kubectl exec -it test-pod -- nc -zv <target-ip> <port>	需安装netcat
路由追踪	> kubectl exec -it test-pod -- traceroute -n -T -p <port> <target-ip>	TCP协议追踪
抓包分析	> kubectl exec -it <pod> -- tcpdump -i eth0 -w /tmp/debug.pcap	需挂载Volume

```

# 私有GKE集群架构与网络设计指南
## 一、核心特性
1.1 基础设计原则
```
特性	私有集群实现方式	安全优势
节点网络	仅分配RFC 1918内部IP	避免公网暴露
Pod通信	使用VPC次级IP范围	网络隔离
互联网访问	默认禁止出站，需通过Cloud NAT代理	防止意外数据泄露
控制平面访问	私有端点(可选配公共端点)	最小化攻击面
```
1.2 典型架构图
```
graph TB
    subgraph VPC网络
        A[私有节点] -->|内部IP| B[Cloud NAT]
        A -->|专用通道| C[控制平面]
        C -->|Private Google Access| D[Google APIs]
        B -->|SNAT| E[互联网]
    end
```
## 二、关键配置实践
2.1 集群创建命令
```
gcloud container clusters create private-cluster
--enable-private-nodes
--master-ipv4-cidr=172.16.0.0/28
--enable-ip-alias
--create-subnetwork name=my-subnet,range=10.0.0.0/20
--enable-master-authorized-networks
--master-authorized-networks=192.168.1.0/24
```
参数说明：
```
--master-ipv4-cidr：控制平面专用IP段（需不与现有网络重叠）
--enable-master-authorized-networks：限定可访问控制平面的IP范围
```
2.2 网络地址规划建议
```
资源类型	CIDR示例	分配方式	建议预留空间
节点IP	10.0.0.0/24	自动从子网分配	+20%
Pod IP	10.4.0.0/16	每个节点分配/24	+30%
Service IP	10.96.0.0/20	集群级别统一分配	+50%
```
## 三、网络访问控制
3.1 出站访问方案对比
```
方案	配置命令示例	适用场景
Cloud NAT	> gcloud compute routers nats create ...	常规互联网访问
Private Google Access	> gcloud compute networks subnets update ...	仅需访问Google服务
代理服务器	需配置http_proxy环境变量	需要流量审计
```
3.2 关键安全配置
- 禁用公共端点：
```
gcloud container clusters update [CLUSTER_NAME]
--disable-public-endpoint
```
- 启用网络策略：
```
gcloud container clusters create [CLUSTER_NAME]
--enable-network-policy
```
- 子网级访问控制：
```
gcloud compute firewall-rules create deny-external
--network=[VPC_NETWORK]
--direction=EGRESS
--destination-ranges=0.0.0.0/0
--deny
```
### 四、运维实践
4.1 常见问题排查
场景：Pod无法访问Google APIs
检查Private Google Access状态：
```
gcloud compute networks subnets describe [SUBNET]
--region=[REGION] | grep privateIpGoogleAccess
```
验证DNS解析：
```
kubectl run -it --rm --image=curlimages/curl testpod --
curl -v https://storage.googleapis.com
```
检查Cloud NAT日志：
```
gcloud logging read
'resource.type="nat_gateway" AND jsonPayload.vpc.src_ip="[POD_IP]"'
```
4.2 监控指标
关键Prometheus指标
```
# NAT转换成功率
target: 'sum(rate(nat_gateway_packets_translated_success[5m])) / sum(rate(nat_gateway_packets_total[5m]))'
threshold: < 0.95 # 低于95%告警

# 私有API访问延迟
target: 'histogram_quantile(0.99, rate(api_request_duration_seconds_bucket{service="googleapis.com"}[5m]))'
threshold: > 2s # 超过2秒告警
```
## 五、最佳实践建议
IP规划原则：
```
节点、Pod、Service CIDR需无重叠
预留至少20%的IP空间用于扩展
```
混合访问模式：
图表
```
graph LR
    A[开发人员] -->|VPN/专用连接| B[堡垒主机]
    B -->|授权IP范围| C[控制平面]
    C --> D[私有节点]
```
版本兼容性：
```
GKE 1.25+ 支持共享VPC中的自动NAT配置
1.21+ 支持每个节点池独立配置NAT
```

# GKE私有集群创建与访问控制实验
## 一、实验目标
1. 创建两种私有集群配置：
  - 完全私有集群：无公共端点（gke-deep-dive）
  - 有限公共访问集群（gke-deep-dive-public）
## 二、核心配置对比
```
配置项	完全私有集群	有限公共访问集群
控制平面端点	仅私有IP	公共IP+授权网络
节点IP类型	仅内部IP	仅内部IP
子网创建方式	自动创建	复用已有子网
典型应用场景	高安全需求环境	需要临时管理访问的场景
```
## 三、详细操作步骤
3.1 创建完全私有集群
```
# 设置计算区域
> gcloud config set compute/zone us-west1-a

# 创建集群（关键参数说明见下表）
> gcloud container clusters create gke-deep-dive \
  --num-nodes=1 \
  --disk-type=pd-standard \
  --disk-size=10 \
  --create-subnetwork name=gke-deep-dive-subnet \
  --enable-ip-alias \
  --enable-private-nodes \
  --enable-private-endpoint \
  --master-ipv4-cidr 172.16.0.32/28
```
- 参数说明表
```
参数	作用
--create-subnetwork	自动创建名为gke-deep-dive-subnet的子网
--enable-ip-alias	启用VPC原生网络（Pod直接使用VPC IP）
--enable-private-nodes	节点不分配公网IP
--enable-private-endpoint	控制平面仅使用私有IP
--master-ipv4-cidr	指定控制平面专用IP段（需不与现有网络重叠）
```
- 3.2 验证配置
```
# 检查节点无公网IP
> gcloud container clusters describe gke-deep-dive | grep -A 3 "nodeConfig"

# 查看子网自动分配情况
> gcloud compute networks subnets list | grep gke-deep-dive
```
3.3 有限公共访问集群创建
```
# 创建带公共端点的私有集群
> gcloud container clusters create gke-deep-dive-public \
  --num-nodes=1 \
  --subnetwork gke-deep-dive-subnet \
  --enable-private-nodes \
  --enable-ip-alias \
  --master-ipv4-cidr 172.16.0.16/28 \
  --enable-master-authorized-networks
```
# 授权Cloud Shell访问
```
# 获取当前Cloud Shell公网IP
> MY_IP=$(dig +short myip.opendns.com @resolver1.opendns.com)

# 更新授权网络
> gcloud container clusters update gke-deep-dive-public \
  --master-authorized-networks $MY_IP/32

# 验证授权
> gcloud container clusters describe gke-deep-dive-public \
  --format "flattened(masterAuthorizedNetworksConfig.cidrBlocks[])"
```
## 四、关键访问控制机制
- 4.1 默认允许的IP范围
  1. 子网主IP范围（节点通信）
  2. Pod使用的次要IP范围
  3. 手动授权的公网IP（仅限公共端点集群）

- 4.2 连接测试技巧
```
# 使用内部IP获取凭证
> gcloud container clusters get-credentials gke-deep-dive-public --internal-ip

# 若连接失败尝试启用全局访问
> gcloud container clusters update gke-deep-dive-public \
  --enable-master-global-access
```
## 五、清理资源
```
# 删除所有实验集群
> gcloud container clusters delete -q gke-deep-dive-public gke-deep-dive \
  --zone us-west1-a
```
## 六、故障排查指南
```
问题现象	可能原因	解决方案
kubectl连接超时	控制平面端点未授权当前IP	更新master-authorized-networks
Pod无法访问Google服务	未启用Private Google Access	检查子网配置：gcloud compute networks subnets describe
节点无法下载容器镜像	缺少NAT网关	创建Cloud NAT网关
```
```
附录：命令速查表
功能	命令
检查集群网络配置	gcloud container clusters describe [NAME]
查看已授权网络	gcloud container clusters describe --format="table(masterAuthorizedNetworksConfig)"
临时增加公网访问	gcloud container clusters update [NAME] --enable-master-authorized-networks --master-authorized-networks x.x.x.x/32
```

# GKE 集群网络路由类型详解
## 一、两种集群网络模型对比
```
特性	VPC-native 集群	基于路由的集群
IP分配方式	使用别名IP地址范围	使用VPC主IP地址范围
路由机制	通过VPC原生路由	依赖自定义静态路由
Pod IP可达性	可在对等VPC中直接访问	需要手动配置路由
防火墙规则粒度	可针对Pod IP制定规则	只能针对节点IP制定规则
路由配额消耗	不消耗静态路由配额	消耗自定义路由配额
典型应用场景	需要精细网络策略的多租户环境	传统网络架构迁移场景
```
## 二、VPC-native 集群核心技术
- 2.1 别名IP工作原理
```
graph TB
    A[Pod1] -->|10.4.1.2| B[节点VM]
    C[Pod2] -->|10.4.1.3| B
    B -->|主IP: 10.0.0.2| D[VPC网络]
    style A fill:#cff,stroke:#333
    style C fill:#cff,stroke:#333
    style B fill:#fcf,stroke:#333
```
  - 关键优势：
    1. 每个Pod获得唯一可路由的IP
    2. 无需配置节点上的SNAT/DNAT
    3. 支持跨VPC对等连接直接访问Pod
- 2.2 创建VPC-native集群
```
# 标准创建命令
> gcloud container clusters create vpc-native-cluster \
  --enable-ip-alias \
  --create-subnetwork name=my-subnet,range=10.0.0.0/20 \
  --cluster-secondary-range-name=pods-range \
  --services-secondary-range-name=svc-range

# 查看已分配的IP范围
> gcloud container clusters describe vpc-native-cluster \
  --format="table(ipAllocationPolicy)"
```
```
CLUSTER_IPV4_CIDR_BLOCK  SERVICES_IPV4_CIDR_BLOCK
10.4.0.0/14             10.96.0.0/20
```
## 三、基于路由的集群限制
### 3.1 主要约束
1. 路由配额限制：
  - 每个VPC网络默认限制为250条自定义路由
  - 大型集群可能快速耗尽配额
2. 网络策略限制：
```
# 无法直接针对Pod IP制定规则
> gcloud compute firewall-rules create allow-pod \
  --source-ranges=10.4.1.2/32  # 此规则对基于路由的集群无效
```
3. 跨VPC访问复杂度：
  - 需要为每个Pod网段手动添加路由
  - 不支持通过Private Service Connect直接访问Pod

## 四、生产环境选型建议
- 4.1 必须使用VPC-native的场景
  1. 需要对接NEG(Network Endpoint Groups)
  2. 使用GKE Autopilot模式
  3. 跨项目/跨VPC的Pod通信需求
  4. 需要Pod级别的防火墙规则
- 4.2 混合架构示例
  ```
  graph LR
    A[VPC-native集群] -->|对等连接| B[共享VPC]
    B -->|VPN隧道| C[本地数据中心]
    A -->|NEG集成| D[Cloud Load Balancer]
  ```
## 五、运维注意事项
- 5.1 IP规划原则
```
资源类型	建议范围大小	备注
节点网络	/24 (256个IP)	每个区域独立子网
Pod网络	/16 (65536个IP)	按节点池划分次级范围
Service网络	/20 (4096个IP)	全集群共享
```
- 5.2 监控指标
```
# VPC-native集群特有指标
- gke_container_network_alias_ip_allocated
- gke_container_network_routes_used

# 路由配额告警
- gce_route_count / 250 > 0.8  # 超过80%配额时触发
```
## 六、迁移指南
- 6.1 从基于路由迁移到VPC-native
前置检查：
```
# 检查现有路由数量
> gcloud compute routes list --filter="network=default" | wc -l
```
- 迁移步骤：
```
graph TD
    A[创建新VPC-native集群] --> B[逐步迁移工作负载]
    B --> C[验证网络策略]
    C --> D[下线旧集群]
```
- 回滚方案：
  - 保持双集群并行运行48小时
  - 配置DNS权重切换

# GKE VPC-native 集群创建实践指南
## 一、前置准备
- 1.1 环境要求
  - 启用以下Google Cloud API：
    - Kubernetes Engine API (container.googleapis.com)
    - Compute Engine API (compute.googleapis.com)
- 1.2 网络规划建议
```
资源类型	CIDR范围示例	最小大小要求	建议大小
节点子网	10.10.0.0/24	/24	/22
Pod IP范围	10.64.0.0/21	/21	/19
Service IP范围	10.96.0.0/21	/21	/20
```
## 二、详细操作步骤
- 2.1 创建自定义VPC网络
```
# 创建自定义模式VPC
> gcloud compute networks create gke-deep-dive-vpc --subnet-mode=custom

# 创建子网（区域级）
> gcloud compute networks subnets create gke-deep-dive-vpc-subnet \
  --network=gke-deep-dive-vpc \
  --range=10.10.0.0/24 \
  --region=us-west1
```
- 2.2 创建VPC-native集群
```
# 创建集群（关键参数说明见下表）
> gcloud container clusters create gke-deep-dive \
  --zone us-west1-a \
  --num-nodes=1 \
  --disk-type=pd-standard \
  --disk-size=10 \
  --enable-ip-alias \
  --network=gke-deep-dive-vpc \
  --subnetwork=gke-deep-dive-vpc-subnet \
  --cluster-ipv4-cidr=/21 \
  --services-ipv4-cidr=/21
```
- 参数说明表
```
参数	作用
--enable-ip-alias	启用VPC-native模式
--cluster-ipv4-cidr	指定Pod IP范围（必须≥/21）
--services-ipv4-cidr	指定Service IP范围（必须≥/21）
--network/subnetwork	指定自定义网络拓扑
```
- 2.3 验证配置
```
# 检查集群网络模式
> gcloud container clusters describe gke-deep-dive \
  --zone us-west1-a \
  --format="value(ipAllocationPolicy.useIpAliases)"

# 查看自动创建的次要IP范围
> gcloud compute networks subnets describe gke-deep-dive-vpc-subnet \
  --region=us-west1 \
  --format="flattened(secondaryIpRanges)"
```
## 三、关键注意事项
- 3.1 IP地址规划原则
  1. 避免重叠：确保Pod/Service CIDR不与以下范围冲突：
    - 节点子网范围
    - 现有VPC中的其他范围
    - 对等连接的VPC范围
  2. 预留扩展空间：
    - 每节点默认需要110个Pod IP地址
    - å预留至少20%的缓冲空间
- 3.2 生产环境建议
```
graph TD
    A[规划阶段] --> B[计算节点规模]
    B --> C[确定Pod IP需求]
    C --> D[选择CIDR块大小]
    D --> E[创建测试集群验证]
    E --> F[调整后部署生产]
```
## 四、清理资源
```
# 删除集群（保留网络)
> gcloud container clusters delete gke-deep-dive \
  --zone us-west1-a

# 完全清理（删除VPC)
> gcloud compute networks subnets delete gke-deep-dive-vpc-subnet \
  --region=us-west1
> gcloud compute networks delete gke-deep-dive-vpc
```
## 五、常见问题处理
- 5.1 创建失败场景分析
```
错误现象	可能原因	解决方案
CIDR大小不足	指定的CIDR小于/21	扩大CIDR范围或使用自动分配
子网IP空间不足	节点数超出子网容量	扩大子网或使用多个区域子网
权限拒绝	缺少Network Admin角色	添加角色：roles/compute.networkAdmin
```
- 5.2 后期扩容建议
1. Pod IP扩容：
```
# 更新现有子网的次要范围
> gcloud compute networks subnets update gke-deep-dive-vpc-subnet \
  --region=us-west1 \
  --add-secondary-ranges="pods-range=10.64.8.0/21"
```
2. 节点扩容：
```
# 增加节点池
> gcloud container node-pools create larger-pool \
  --cluster=gke-deep-dive \
  --num-nodes=3 \
  --zone=us-west1-a
```

### 附录：命令速查表
```
功能	命令
查看集群网络配置	gcloud container clusters describe [NAME] --format="json(networkConfig)"
获取已使用的Pod IP	kubectl get pods -o jsonpath='{.items[*].status.podIP}' | tr ' ' '\n' | sort | uniq -c
检查路由表	gcloud compute routes list --filter="network=gke-deep-dive-vpc"
```