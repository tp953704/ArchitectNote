# Kafka Streams 深度解析筆記

這份筆記專為希望深入理解 Kafka Streams 的開發者而設計。我們將從「為什麼需要它」出發，逐步深入其核心概念、架構原理和實戰應用。

---

## 第一章：為什麼需要 Kafka Streams？

當數據流入 Kafka 後，我們自然需要處理它。Kafka 提供了三種主要的工具：

1.  **Consumer API**：最基礎的 API，靈活度最高。
2.  **Kafka Streams API**：一個強大的客戶端庫，用於構建流處理應用。
3.  **ksqlDB (前身為 KSQL)**：使用 SQL 語法進行流處理。

### 1.1 Consumer API 的局限性

使用 Consumer API 可以實現簡單的流處理，例如讀取一個 Topic 的數據，做一些簡單的驗證，然後寫入另一個 Topic。但一旦場景變得複雜，Consumer API 就會顯得力不從心。

> **場景舉例：病患血氧飽和度監測**
> 想像一個應用，它需要接收來自病患監測儀的即時血氧（SpO2）讀數流。我們的目標是：**計算過去 5 秒內讀數的平均值**，並將結果發送到一個新的 Topic，用於生成趨勢圖表。

如果用 Consumer API 來實現，你需要自己處理一大堆麻煩事：
*   **窗口管理 (Windowing)**：你需要自己寫程式碼來定義和管理「5 秒」這個時間窗口。
*   **狀態管理 (State Management)**：你需要一個地方來存儲這個窗口內的數據（比如一個 List 或 Map），並計算平均值。
*   **容錯 (Fault Tolerance)**：如果你的應用在計算過程中崩潰了，內存中的狀態（那個窗口內的數據）就全丟了。重啟後如何恢復到崩潰前的狀態？這非常棘手。
*   **並行處理 (Parallelism)**：如何擴展你的應用來處理成千上萬個病患的數據流？你需要手動管理多個 Consumer 和它們的狀態。

這些問題會讓你的業務邏輯被大量樣板程式碼 (Boilerplate Code) 所淹沒，而且很容易出錯。

### 1.2 Kafka Streams：優雅的解決方案

Kafka Streams 正是為了解決上述痛點而生的。它是一個 **客戶端庫 (Client Library)**，而不是一個像 Spark 或 Flink 那樣的重量級框架。這意味著：

*   **易於整合**：你可以像引入任何其他 Java/Scala 庫一樣，將它加入到你的應用程式中。
*   **無中心化架構**：你的流處理應用就是一個普通的 Java 應用，不需要一個專門的計算集群來運行它。你可以像啟動其他服務一樣啟動它。
*   **內置高級功能**：它原生支持狀態管理、窗口操作、聚合、Join 等複雜功能，讓你專注於業務邏輯。

---

## 第二章：核心概念：流 (Stream) 與表 (Table)

要精通 Kafka Streams，必須理解其最重要的核心抽象：**流 (Stream)** 和 **表 (Table)**，以及它們之間的 **二元性 (Duality)**。

### 2.1 KStream：不可變的事件流

**KStream** 代表一個 **事件記錄流 (record stream)**。每一條記錄都是一個獨立的、不可變的事實。你可以對它進行新增，但不能修改已有的記錄。

> **比喻：銀行交易流水**
> 一個銀行的交易流水就是一個典型的 KStream。每一筆交易（存款 $100、取款 $50）都是一個獨立的事件，按時間順序記錄下來。你只會不斷追加新的交易，而不會去修改歷史交易記錄。

### 2.2 KTable：可變的狀態快照

**KTable** 代表一個 **變更日誌流 (changelog stream)**。它被解讀為在某個時間點上，每個 Key 的 **最新狀態快照**。對於同一個 Key，新的記錄會 **覆蓋** 舊的記錄。

> **比喻：銀行帳戶餘額**
> 你的銀行帳戶餘額就是一個 KTable。它只關心每個帳戶（Key）當前的最終金額是多少。當一筆新交易發生時，餘額會被更新成最新的值。

### 2.3 流-表二元性 (Stream-Table Duality)

**流和表其實是同一枚硬幣的兩面。**

*   **你可以從一個 KStream 聚合 (aggregate) 出一個 KTable**。
    *   **例子**：對「銀行交易流水 (KStream)」按帳戶進行求和，就可以得到「帳戶餘額表 (KTable)」。
*   **你可以將一個 KTable 的更新轉換為一個 KStream**。
    *   **例子**：每當「帳戶餘額表 (KTable)」發生變更時，將這個變更事件（例如：帳戶 A 的餘額從 $500 更新為 $600）作為一條新的記錄，就形成了一個新的事件流 (KStream)。

理解這種二元性，是掌握 Kafka Streams 中 Join、聚合等高級操作的基礎。

---

## 第三章：核心操作 (DSL API)

Kafka Streams 提供了一套類似 Java Stream API 的高階 DSL (Domain-Specific Language)，讓你可以用鏈式調用的方式構建處理拓撲。

### 3.1 無狀態操作 (Stateless Operations)

這類操作處理每條消息都是獨立的，不需要記住之前的任何信息。

*   `map(KeyValueMapper)` / `mapValues(ValueMapper)`：對每條記錄的 Key/Value 進行轉換。
*   `flatMap(KeyValueMapper)` / `flatMapValues(ValueMapper)`：一對多轉換，將一條記錄轉換為零條或多條記錄。
*   `filter(Predicate)`：過濾消息，只保留滿足條件的記錄。
*   `peek(ForeachAction)`：對流中的每條記錄執行一個動作，通常用於調試或日誌記錄，它不會修改流本身。
*   `branch(Predicate...)`：將一個流根據多個條件分割成多個流。

### 3.2 有狀態操作 (Stateful Operations)

這類操作需要記住歷史信息才能完成計算，Kafka Streams 會自動為你管理這些「狀態」。

*   `groupByKey()` / `groupBy(KeyValueMapper)`：按 Key 對記錄進行分組，是進行聚合操作的前提。
*   **聚合 (Aggregations)**：
    *   `count()`：計算每個 Key 的記錄數。
    *   `aggregate(Initializer, Aggregator)`：通用的聚合操作，可以實現求和、求平均等。
    *   `reduce(Reducer)`：`aggregate` 的一個特例，當輸出類型和輸入類型相同時使用。
*   **連接 (Joins)**：
    *   `KStream-KStream Join`：將兩個流在一個時間窗口內進行連接。
    *   `KStream-KTable Join`：用流中的數據去查詢表中的狀態。
    *   `KTable-KTable Join`：將兩個表進行連接。
*   **窗口 (Windowing)**：
    *   `windowedBy(Windows)`：將流按時間切分成不同的窗口，是進行時間相關聚合的基礎。支持滾動窗口、滑動窗口、會話窗口等。

---

## 第四章：架構與原理

### 4.1 應用架構：一個帶腦子的消費者

一個 Kafka Streams 應用本質上就是一個 **內嵌了處理邏輯的 Kafka 消費者**。

*   **並行模型**：它完全利用了 Kafka 的 Consumer Group 機制。如果你啟動了多個應用實例（Instance），它們會組成一個 Consumer Group。Kafka 的分區會被自動分配給這些實例，從而實現負載均衡和並行處理。一個 Topic 的 Partition 越多，你的應用就能並行擴展到越多的實例。
*   **任務 (Task)**：在應用內部，每個被分配到的 Topic Partition 都會對應一個 **Task**。這個 Task 就是執行你定義的處理邏輯（拓撲）的最小單元。

### 4.2 狀態管理與容錯

這是 Kafka Streams 最強大的功能之一。

*   **本地狀態存儲 (Local State Store)**：對於有狀態操作（如 `count`, `aggregate`），Kafka Streams 會在執行任務的應用實例本地創建一個 **狀態存儲**（默認使用 RocksDB，一個高性能的嵌入式 KV 存儲）。所有計算和查詢都在本地進行，速度極快。

*   **變更日誌主題 (Changelog Topic)**：僅有本地狀態是不夠的，如果應用實例崩潰，本地的 RocksDB 文件可能會損壞或丟失。為此，Kafka Streams 會將本地狀態的 **每一次變更** 都發送到一個內部的 Kafka Topic，這個 Topic 就叫 **Changelog Topic**。

*   **容錯恢復**：當一個崩潰的應用實例在另一台機器上重啟時，它會：
    1.  被分配到一個或多個 Task。
    2.  為每個 Task 創建一個空的本地狀態存儲。
    3.  從對應的 Changelog Topic 中讀取所有歷史變更記錄，並在本地狀態存儲中「重放」這些變更。
    4.  一旦狀態完全恢復，它就開始處理上游 Topic 的新數據。

這個機制確保了即使發生故障，應用的狀態也能被完整恢復，實現了高容錯。

---

## 第五章：實戰範例：Word Count

讓我們用最經典的 Word Count 例子來看看 Kafka Streams 程式碼的樣子。

**目標**：從一個名為 `streams-plaintext-input` 的 Topic 讀取文本行，計算每個單詞出現的次數，並將結果 `(單詞, 次數)` 寫入到 `streams-wordcount-output` Topic。

```java
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.kstream.KTable;
import org.apache.kafka.streams.kstream.Produced;

import java.util.Arrays;
import java.util.Properties;
import java.util.concurrent.CountDownLatch;

public class WordCountApplication {

    public static void main(String[] args) {
        Properties props = new Properties();
        // 每個 Streams 應用必須有唯一的 application.id
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "wordcount-application");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        // 指定默認的 Key 和 Value 的序列化/反序列化器
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());

        StreamsBuilder builder = new StreamsBuilder();

        // 1. 從源 Topic 創建一個 KStream
        KStream<String, String> textLines = builder.stream("streams-plaintext-input");

        // 2. 進行處理
        KTable<String, Long> wordCounts = textLines
                // 將每行文本按空格拆分成單詞 (flatMapValues)
                .flatMapValues(textLine -> Arrays.asList(textLine.toLowerCase().split("\\W+")))
                // 按單詞 (Value) 進行分組，需要先將單詞設置為 Key
                .groupBy((key, word) -> word)
                // 計算每個分組的數量
                .count();

        // 3. 將結果 (KTable) 寫入到目標 Topic
        wordCounts.toStream().to("streams-wordcount-output", Produced.with(Serdes.String(), Serdes.Long()));

        // 創建並啟動 KafkaStreams 實例
        final KafkaStreams streams = new KafkaStreams(builder.build(), props);
        
        // ... (省略了優雅關閉的程式碼)

        streams.start();
    }
}
```

---

## 結語

Kafka Streams 是一個功能強大且設計優雅的工具。它將流處理的複雜性（如狀態管理和容錯）進行了完美的封裝，讓開發者可以專注於實現業務價值。通過理解其流-表二元性的核心思想和底層的架構原理，你將能夠充滿信心地構建高效、可靠的即時應用程式。

```